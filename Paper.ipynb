{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb8620b7-623c-419a-bee1-ed8172a6cfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: merged_dataset.csv\n",
      "Loaded dataframe.\n",
      "\n",
      "=== Basic info ===\n",
      "Shape: (16004, 8)\n",
      "Columns: ['t', 'Va', 'Vb', 'Vc', 'Ia', 'Ib', 'Ic', 'Fault']\n",
      "\n",
      "Dtypes:\n",
      " t        float64\n",
      "Va       float64\n",
      "Vb       float64\n",
      "Vc       float64\n",
      "Ia       float64\n",
      "Ib       float64\n",
      "Ic       float64\n",
      "Fault      int64\n",
      "dtype: object\n",
      "\n",
      "First 5 rows:\n",
      "      t         Va          Vb         Vc          Ia            Ib          Ic  Fault\n",
      "0.0000 123.673155 -318.714610 195.041455 3827.709068  -9954.681312 6126.972245      0\n",
      "0.0001 132.928422 -319.851525 186.923103 4117.440004  -9991.463731 5874.023726      0\n",
      "0.0002 142.052661 -320.672826 178.620166 4403.112401 -10018.387070 5615.274666      0\n",
      "0.0003 151.036833 -321.177694 170.140861 4684.443276 -10035.424470 5350.981195      0\n",
      "0.0004 159.872046 -321.365624 161.493578 4961.154162 -10042.558910 5081.404750      0\n",
      "\n",
      "Last 5 rows:\n",
      "      t         Va          Vb         Vc          Ia           Ib          Ic  Fault\n",
      "0.3996  85.518605 -311.035890 225.517284 2633.734420 -9709.769145 7076.034724      0\n",
      "0.3997  95.206804 -313.421290 218.214485 2936.845503 -9785.539018 6848.693515      0\n",
      "0.3998 104.801046 -315.497381 210.696335 3237.058271 -9851.651747 6614.593475      0\n",
      "0.3999 114.291861 -317.262114 202.970253 3534.076451 -9908.042084 6373.965633      0\n",
      "0.4000 123.669884 -318.713748 195.043864 3827.606921 -9954.654380 6127.047459      0\n",
      "\n",
      "Missing values per column:\n",
      "t        0\n",
      "Va       0\n",
      "Vb       0\n",
      "Vc       0\n",
      "Ia       0\n",
      "Ib       0\n",
      "Ic       0\n",
      "Fault    0\n",
      "dtype: int64\n",
      "\n",
      "Unique Fault labels and counts:\n",
      "Fault\n",
      "0    10001\n",
      "2     2001\n",
      "3     2001\n",
      "1     2001\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sampling interval (seconds):\n",
      " count: 16000\n",
      " mean: 0.000100\n",
      " median: 0.000100\n",
      " std: 0.000000\n",
      " approx sampling frequency (Hz) from median dt: 10000.000 Hz\n",
      "\n",
      "Saved waveform overview to: eda_outputs\\waveform_overview.png\n",
      "Saved waveform zoom to: eda_outputs\\waveform_zoom.png\n",
      "Saved spectrogram to: eda_outputs\\spectrogram_faulted_phase.png\n",
      "\n",
      "Quick channel statistics (first 1s vs last 1s if t present):\n",
      "Va: first mean=3.892e-05, std=227.2, rms=227.2 | last mean=0.06968, std=129.9, rms=129.9\n",
      "Vb: first mean=-9.94e-06, std=227.2, rms=227.2 | last mean=0.06985, std=229.4, rms=229.4\n",
      "Vc: first mean=-2.897e-05, std=227.2, rms=227.2 | last mean=0.06891, std=200.5, rms=200.5\n",
      "Ia: first mean=-0.001669, std=7101, rms=7101 | last mean=1.704, std=3.489e+04, rms=3.489e+04\n",
      "Ib: first mean=-0.0003109, std=7101, rms=7101 | last mean=-1.896, std=2.761e+04, rms=2.761e+04\n",
      "Ic: first mean=0.001979, std=7101, rms=7101 | last mean=0.1924, std=3.713e+04, rms=3.713e+04\n",
      "\n",
      "Step 1 complete. Please paste the printed console output here and confirm whether the files in the 'eda_outputs' folder were created (waveform_overview.png, waveform_zoom.png, spectrogram_faulted_phase.png).\n"
     ]
    }
   ],
   "source": [
    "# step1_eda_lg_fault.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "\n",
    "# === CONFIG ===\n",
    "CSV_PATH = \"merged_dataset.csv\"   # change if your file path differs\n",
    "OUT_DIR = \"eda_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# === 1. Load ===\n",
    "print(\"Loading:\", CSV_PATH)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Loaded dataframe.\")\n",
    "\n",
    "# === 2. Basic info ===\n",
    "print(\"\\n=== Basic info ===\")\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"\\nDtypes:\\n\", df.dtypes)\n",
    "print(\"\\nFirst 5 rows:\\n\", df.head().to_string(index=False))\n",
    "print(\"\\nLast 5 rows:\\n\", df.tail().to_string(index=False))\n",
    "\n",
    "# Missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Unique labels for Fault\n",
    "if \"Fault\" in df.columns:\n",
    "    print(\"\\nUnique Fault labels and counts:\")\n",
    "    print(df[\"Fault\"].value_counts(dropna=False))\n",
    "else:\n",
    "    print(\"\\nColumn 'Fault' not found in CSV. Please confirm column names.\")\n",
    "\n",
    "# === 3. Sampling interval estimation ===\n",
    "if \"t\" in df.columns:\n",
    "    t = df[\"t\"].values.astype(float)\n",
    "    dt = np.diff(t)\n",
    "    # filter out zero or negative dt if any\n",
    "    dt = dt[np.isfinite(dt) & (dt>0)]\n",
    "    if len(dt) == 0:\n",
    "        print(\"\\nNo positive time differences found in 't' to estimate sampling interval.\")\n",
    "    else:\n",
    "        print(\"\\nSampling interval (seconds):\")\n",
    "        print(\" count:\", len(dt))\n",
    "        print(\" mean: {:.6f}\".format(np.mean(dt)))\n",
    "        print(\" median: {:.6f}\".format(np.median(dt)))\n",
    "        print(\" std: {:.6f}\".format(np.std(dt)))\n",
    "        # approximate sampling frequency\n",
    "        fs_median = 1.0 / np.median(dt)\n",
    "        print(\" approx sampling frequency (Hz) from median dt: {:.3f} Hz\".format(fs_median))\n",
    "else:\n",
    "    print(\"\\nColumn 't' not found in CSV. Please confirm column names.\")\n",
    "\n",
    "# === 4. Quick plot: full time-series overview (stacked) ===\n",
    "channels_v = [c for c in df.columns if c.lower().startswith(\"v\")]\n",
    "channels_i = [c for c in df.columns if c.lower().startswith(\"i\")]\n",
    "channels = channels_v + channels_i\n",
    "\n",
    "if len(channels) == 0:\n",
    "    print(\"\\nNo Va/Vb/Vc/Ia/Ib/Ic columns detected by prefix. Detected columns:\", df.columns.tolist())\n",
    "else:\n",
    "    fig, axs = plt.subplots(len(channels), 1, figsize=(14, 2.5*len(channels)), sharex=True)\n",
    "    for ax, ch in zip(axs, channels):\n",
    "        ax.plot(df[\"t\"], df[ch], linewidth=0.5)\n",
    "        ax.set_ylabel(ch)\n",
    "        ax.grid(True, linewidth=0.3)\n",
    "    axs[-1].set_xlabel(\"t (s)\")\n",
    "    fig.suptitle(\"Waveform overview\")\n",
    "    out1 = os.path.join(OUT_DIR, \"waveform_overview.png\")\n",
    "    plt.tight_layout(rect=[0,0,1,0.96])\n",
    "    plt.savefig(out1, dpi=200)\n",
    "    plt.close(fig)\n",
    "    print(\"\\nSaved waveform overview to:\", out1)\n",
    "\n",
    "# === 5. Zoomed-in window near start (first N seconds) ===\n",
    "try:\n",
    "    t0 = df[\"t\"].iloc[0]\n",
    "    N_sec = min(0.05, (df[\"t\"].iloc[-1] - t0) * 0.1)  # default 50 ms or 10% of duration\n",
    "    mask = (df[\"t\"] >= t0) & (df[\"t\"] <= t0 + N_sec)\n",
    "    if mask.sum() < 10:\n",
    "        # fallback: first 100 samples\n",
    "        mask = df.index < min(100, len(df))\n",
    "    fig, axs = plt.subplots(len(channels), 1, figsize=(14, 2.5*len(channels)), sharex=True)\n",
    "    for ax, ch in zip(axs, channels):\n",
    "        ax.plot(df.loc[mask, \"t\"], df.loc[mask, ch], linewidth=0.7)\n",
    "        ax.set_ylabel(ch)\n",
    "        ax.grid(True, linewidth=0.3)\n",
    "    axs[-1].set_xlabel(\"t (s)\")\n",
    "    fig.suptitle(\"Waveform zoom (start)\")\n",
    "    out2 = os.path.join(OUT_DIR, \"waveform_zoom.png\")\n",
    "    plt.tight_layout(rect=[0,0,1,0.96])\n",
    "    plt.savefig(out2, dpi=200)\n",
    "    plt.close(fig)\n",
    "    print(\"Saved waveform zoom to:\", out2)\n",
    "except Exception as e:\n",
    "    print(\"Could not create zoom plot:\", e)\n",
    "\n",
    "# === 6. Spectrogram of the (assumed) faulted phase ===\n",
    "# Default to Va if present, otherwise first voltage column\n",
    "spec_ch = None\n",
    "for c in [\"Va\",\"VA\",\"va\",\"Va \"]:\n",
    "    if c in df.columns:\n",
    "        spec_ch = c\n",
    "        break\n",
    "if spec_ch is None and len(channels_v) > 0:\n",
    "    spec_ch = channels_v[0]\n",
    "\n",
    "if spec_ch is not None:\n",
    "    sig = df[spec_ch].values\n",
    "    # if t available, compute fs, else assume 10000 Hz for spectrogram default (but warn)\n",
    "    if \"t\" in df.columns:\n",
    "        dt = np.median(np.diff(df[\"t\"].values.astype(float)))\n",
    "        if dt <= 0:\n",
    "            fs = None\n",
    "        else:\n",
    "            fs = 1.0 / dt\n",
    "    else:\n",
    "        fs = None\n",
    "\n",
    "    if fs is None:\n",
    "        print(\"\\nCould not determine sampling frequency from 't'. Spectrogram will use default fs=10000 Hz.\")\n",
    "        fs = 10000.0\n",
    "\n",
    "    f, tt, Sxx = signal.spectrogram(sig, fs=fs, nperseg=256, noverlap=192, scaling=\"density\", mode=\"magnitude\")\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.pcolormesh(tt, f, np.log1p(Sxx), shading='gouraud')\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.title(f\"Spectrogram (log1p) - {spec_ch} - fs={fs:.1f} Hz\")\n",
    "    plt.colorbar(label='log magnitude')\n",
    "    out3 = os.path.join(OUT_DIR, \"spectrogram_faulted_phase.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out3, dpi=200)\n",
    "    plt.close()\n",
    "    print(\"Saved spectrogram to:\", out3)\n",
    "else:\n",
    "    print(\"\\nNo voltage column found for spectrogram.\")\n",
    "\n",
    "# === 7. Quick statistics per channel (mean, std, RMS) for first and last 1s if available ===\n",
    "print(\"\\nQuick channel statistics (first 1s vs last 1s if t present):\")\n",
    "if \"t\" in df.columns:\n",
    "    tmin, tmax = df[\"t\"].min(), df[\"t\"].max()\n",
    "    dur = tmax - tmin\n",
    "    window = min(1.0, dur/10.0)  # 1s or 10% of duration\n",
    "    first_mask = (df[\"t\"] >= tmin) & (df[\"t\"] < tmin + window)\n",
    "    last_mask = (df[\"t\"] <= tmax) & (df[\"t\"] > tmax - window)\n",
    "else:\n",
    "    first_mask = df.index < min(100, len(df))\n",
    "    last_mask = df.index >= max(0, len(df)-min(100, len(df)))\n",
    "\n",
    "for ch in channels:\n",
    "    def stats(mask):\n",
    "        a = df.loc[mask, ch].dropna().values\n",
    "        if len(a)==0:\n",
    "            return (np.nan, np.nan, np.nan)\n",
    "        return (np.mean(a), np.std(a), np.sqrt(np.mean(a**2)))\n",
    "    fmean, fstd, frms = stats(first_mask)\n",
    "    lmean, lstd, lrms = stats(last_mask)\n",
    "    print(f\"{ch}: first mean={fmean:.4g}, std={fstd:.4g}, rms={frms:.4g} | last mean={lmean:.4g}, std={lstd:.4g}, rms={lrms:.4g}\")\n",
    "\n",
    "print(\"\\nStep 1 complete. Please paste the printed console output here and confirm whether the files in the 'eda_outputs' folder were created (waveform_overview.png, waveform_zoom.png, spectrogram_faulted_phase.png).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e67b734-8d31-4867-8c37-75fb9db78d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: merged_dataset.csv\n",
      "Loaded dataframe.\n",
      "\n",
      "=== Basic info ===\n",
      "Shape: (16004, 8)\n",
      "Columns: ['t', 'Va', 'Vb', 'Vc', 'Ia', 'Ib', 'Ic', 'Fault']\n",
      "\n",
      "Dtypes:\n",
      " t        float64\n",
      "Va       float64\n",
      "Vb       float64\n",
      "Vc       float64\n",
      "Ia       float64\n",
      "Ib       float64\n",
      "Ic       float64\n",
      "Fault      int64\n",
      "dtype: object\n",
      "\n",
      "First 5 rows:\n",
      "      t         Va          Vb         Vc          Ia            Ib          Ic  Fault\n",
      "0.0000 123.673155 -318.714610 195.041455 3827.709068  -9954.681312 6126.972245      0\n",
      "0.0001 132.928422 -319.851525 186.923103 4117.440004  -9991.463731 5874.023726      0\n",
      "0.0002 142.052661 -320.672826 178.620166 4403.112401 -10018.387070 5615.274666      0\n",
      "0.0003 151.036833 -321.177694 170.140861 4684.443276 -10035.424470 5350.981195      0\n",
      "0.0004 159.872046 -321.365624 161.493578 4961.154162 -10042.558910 5081.404750      0\n",
      "\n",
      "Last 5 rows:\n",
      "      t         Va          Vb         Vc          Ia           Ib          Ic  Fault\n",
      "0.3996  85.518605 -311.035890 225.517284 2633.734420 -9709.769145 7076.034724      0\n",
      "0.3997  95.206804 -313.421290 218.214485 2936.845503 -9785.539018 6848.693515      0\n",
      "0.3998 104.801046 -315.497381 210.696335 3237.058271 -9851.651747 6614.593475      0\n",
      "0.3999 114.291861 -317.262114 202.970253 3534.076451 -9908.042084 6373.965633      0\n",
      "0.4000 123.669884 -318.713748 195.043864 3827.606921 -9954.654380 6127.047459      0\n",
      "\n",
      "Missing values per column:\n",
      "t        0\n",
      "Va       0\n",
      "Vb       0\n",
      "Vc       0\n",
      "Ia       0\n",
      "Ib       0\n",
      "Ic       0\n",
      "Fault    0\n",
      "dtype: int64\n",
      "\n",
      "Unique Fault labels and counts:\n",
      "Fault\n",
      "0    10001\n",
      "2     2001\n",
      "3     2001\n",
      "1     2001\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sampling interval (seconds):\n",
      " count: 16000\n",
      " mean: 0.000100\n",
      " median: 0.000100\n",
      " std: 0.000000\n",
      " approx sampling frequency (Hz) from median dt: 10000.000 Hz\n",
      "\n",
      "Saved waveform overview to: eda_outputs\\waveform_overview.png\n",
      "Saved waveform zoom to: eda_outputs\\waveform_zoom.png\n",
      "Saved spectrogram to: eda_outputs\\spectrogram_faulted_phase.png\n",
      "\n",
      "Quick channel statistics (first 1s vs last 1s if t present):\n",
      "Va: first mean=3.892e-05, std=227.2, rms=227.2 | last mean=0.06968, std=129.9, rms=129.9\n",
      "Vb: first mean=-9.94e-06, std=227.2, rms=227.2 | last mean=0.06985, std=229.4, rms=229.4\n",
      "Vc: first mean=-2.897e-05, std=227.2, rms=227.2 | last mean=0.06891, std=200.5, rms=200.5\n",
      "Ia: first mean=-0.001669, std=7101, rms=7101 | last mean=1.704, std=3.489e+04, rms=3.489e+04\n",
      "Ib: first mean=-0.0003109, std=7101, rms=7101 | last mean=-1.896, std=2.761e+04, rms=2.761e+04\n",
      "Ic: first mean=0.001979, std=7101, rms=7101 | last mean=0.1924, std=3.713e+04, rms=3.713e+04\n",
      "\n",
      "Step 1 complete. Check 'eda_outputs/' for IEEE-quality figures.\n"
     ]
    }
   ],
   "source": [
    "# step1_eda_lg_fault_ieee.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "\n",
    "# === CONFIG ===\n",
    "CSV_PATH = \"merged_dataset.csv\"   # change if your file path differs\n",
    "OUT_DIR = \"eda_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# === IEEE Publication Plot Settings ===\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times New Roman\", \"Times\", \"DejaVu Serif\", \"Nimbus Roman No9 L\"],\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelweight\": \"bold\",\n",
    "    \"axes.titleweight\": \"bold\",\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"legend.fontsize\": 11,\n",
    "    \"figure.dpi\": 120,\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"savefig.bbox\": \"tight\",\n",
    "    \"savefig.pad_inches\": 0.05,\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \"grid.linestyle\": \"--\"\n",
    "})\n",
    "\n",
    "# === 1. Load ===\n",
    "print(\"Loading:\", CSV_PATH)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Loaded dataframe.\")\n",
    "\n",
    "# === 2. Basic info ===\n",
    "print(\"\\n=== Basic info ===\")\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"\\nDtypes:\\n\", df.dtypes)\n",
    "print(\"\\nFirst 5 rows:\\n\", df.head().to_string(index=False))\n",
    "print(\"\\nLast 5 rows:\\n\", df.tail().to_string(index=False))\n",
    "\n",
    "# Missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Unique labels for Fault\n",
    "if \"Fault\" in df.columns:\n",
    "    print(\"\\nUnique Fault labels and counts:\")\n",
    "    print(df[\"Fault\"].value_counts(dropna=False))\n",
    "else:\n",
    "    print(\"\\nColumn 'Fault' not found in CSV. Please confirm column names.\")\n",
    "\n",
    "# === 3. Sampling interval estimation ===\n",
    "if \"t\" in df.columns:\n",
    "    t = df[\"t\"].values.astype(float)\n",
    "    dt = np.diff(t)\n",
    "    dt = dt[np.isfinite(dt) & (dt > 0)]\n",
    "    if len(dt) == 0:\n",
    "        print(\"\\nNo positive time differences found in 't' to estimate sampling interval.\")\n",
    "    else:\n",
    "        print(\"\\nSampling interval (seconds):\")\n",
    "        print(\" count:\", len(dt))\n",
    "        print(\" mean: {:.6f}\".format(np.mean(dt)))\n",
    "        print(\" median: {:.6f}\".format(np.median(dt)))\n",
    "        print(\" std: {:.6f}\".format(np.std(dt)))\n",
    "        fs_median = 1.0 / np.median(dt)\n",
    "        print(\" approx sampling frequency (Hz) from median dt: {:.3f} Hz\".format(fs_median))\n",
    "else:\n",
    "    print(\"\\nColumn 't' not found in CSV. Please confirm column names.\")\n",
    "\n",
    "# === 4. Waveform overview ===\n",
    "channels_v = [c for c in df.columns if c.lower().startswith(\"v\")]\n",
    "channels_i = [c for c in df.columns if c.lower().startswith(\"i\")]\n",
    "channels = channels_v + channels_i\n",
    "\n",
    "if len(channels) == 0:\n",
    "    print(\"\\nNo Va/Vb/Vc/Ia/Ib/Ic columns detected by prefix. Detected columns:\", df.columns.tolist())\n",
    "else:\n",
    "    fig, axs = plt.subplots(len(channels), 1, figsize=(10, 2.2 * len(channels)), sharex=True)\n",
    "    for ax, ch in zip(axs, channels):\n",
    "        ax.plot(df[\"t\"], df[ch], linewidth=0.8)\n",
    "        if ch.lower().startswith(\"v\"):\n",
    "            ax.set_ylabel(\"V (Volts)\", fontweight='bold')\n",
    "        elif ch.lower().startswith(\"i\"):\n",
    "            ax.set_ylabel(\"I (Amperes)\", fontweight='bold')\n",
    "        ax.set_title(ch, fontweight='bold', loc='left', fontsize=12)\n",
    "    axs[-1].set_xlabel(\"Time (s)\", fontweight='bold')\n",
    "    fig.suptitle(\"Waveform Overview\", fontweight='bold')\n",
    "    out1 = os.path.join(OUT_DIR, \"waveform_overview.png\")\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.savefig(out1, format=\"png\")\n",
    "    plt.savefig(out1.replace(\".png\", \".pdf\"), format=\"pdf\")\n",
    "    plt.close(fig)\n",
    "    print(\"\\nSaved waveform overview to:\", out1)\n",
    "\n",
    "\n",
    "# === 5. Zoomed waveform ===\n",
    "try:\n",
    "    t0 = df[\"t\"].iloc[0]\n",
    "    N_sec = min(0.05, (df[\"t\"].iloc[-1] - t0) * 0.1)\n",
    "    mask = (df[\"t\"] >= t0) & (df[\"t\"] <= t0 + N_sec)\n",
    "    if mask.sum() < 10:\n",
    "        mask = df.index < min(100, len(df))\n",
    "    fig, axs = plt.subplots(len(channels), 1, figsize=(10, 2.2 * len(channels)), sharex=True)\n",
    "    for ax, ch in zip(axs, channels):\n",
    "        ax.plot(df.loc[mask, \"t\"], df.loc[mask, ch], linewidth=0.9)\n",
    "        if ch.lower().startswith(\"v\"):\n",
    "            ax.set_ylabel(\"V (Volts)\", fontweight='bold')\n",
    "        elif ch.lower().startswith(\"i\"):\n",
    "            ax.set_ylabel(\"I (Amperes)\", fontweight='bold')\n",
    "        ax.set_title(ch, fontweight='bold', loc='left', fontsize=12)\n",
    "    axs[-1].set_xlabel(\"Time (s)\", fontweight='bold')\n",
    "    fig.suptitle(\"Waveform Zoom (Initial 50 ms)\", fontweight='bold')\n",
    "    out2 = os.path.join(OUT_DIR, \"waveform_zoom.png\")\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.savefig(out2, format=\"png\")\n",
    "    plt.savefig(out2.replace(\".png\", \".pdf\"), format=\"pdf\")\n",
    "    plt.close(fig)\n",
    "    print(\"Saved waveform zoom to:\", out2)\n",
    "except Exception as e:\n",
    "    print(\"Could not create zoom plot:\", e)\n",
    "\n",
    "# === 6. Spectrogram of faulted phase ===\n",
    "spec_ch = None\n",
    "for c in [\"Va\", \"VA\", \"va\", \"Va \"]:\n",
    "    if c in df.columns:\n",
    "        spec_ch = c\n",
    "        break\n",
    "if spec_ch is None and len(channels_v) > 0:\n",
    "    spec_ch = channels_v[0]\n",
    "\n",
    "if spec_ch is not None:\n",
    "    sig = df[spec_ch].values\n",
    "    if \"t\" in df.columns:\n",
    "        dt = np.median(np.diff(df[\"t\"].values.astype(float)))\n",
    "        fs = 1.0 / dt if dt > 0 else None\n",
    "    else:\n",
    "        fs = None\n",
    "    if fs is None:\n",
    "        print(\"\\nCould not determine sampling frequency from 't'. Using fs=10000 Hz.\")\n",
    "        fs = 10000.0\n",
    "\n",
    "    f, tt, Sxx = signal.spectrogram(sig, fs=fs, nperseg=256, noverlap=192,\n",
    "                                    scaling=\"density\", mode=\"magnitude\")\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.pcolormesh(tt, f, np.log1p(Sxx), shading='gouraud')\n",
    "    plt.ylabel('Frequency (Hz)', fontweight='bold')\n",
    "    plt.xlabel('Time (s)', fontweight='bold')\n",
    "    plt.title(f\"Spectrogram (log scale) - {spec_ch}\", fontweight='bold')\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label(\"log Magnitude\", fontweight='bold')\n",
    "    out3 = os.path.join(OUT_DIR, \"spectrogram_faulted_phase.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out3, format=\"png\")\n",
    "    plt.savefig(out3.replace(\".png\", \".pdf\"), format=\"pdf\")\n",
    "    plt.close()\n",
    "    print(\"Saved spectrogram to:\", out3)\n",
    "else:\n",
    "    print(\"\\nNo voltage column found for spectrogram.\")\n",
    "\n",
    "# === 7. Quick stats ===\n",
    "print(\"\\nQuick channel statistics (first 1s vs last 1s if t present):\")\n",
    "if \"t\" in df.columns:\n",
    "    tmin, tmax = df[\"t\"].min(), df[\"t\"].max()\n",
    "    dur = tmax - tmin\n",
    "    window = min(1.0, dur / 10.0)\n",
    "    first_mask = (df[\"t\"] >= tmin) & (df[\"t\"] < tmin + window)\n",
    "    last_mask = (df[\"t\"] <= tmax) & (df[\"t\"] > tmax - window)\n",
    "else:\n",
    "    first_mask = df.index < min(100, len(df))\n",
    "    last_mask = df.index >= max(0, len(df) - min(100, len(df)))\n",
    "\n",
    "for ch in channels:\n",
    "    def stats(mask):\n",
    "        a = df.loc[mask, ch].dropna().values\n",
    "        if len(a) == 0:\n",
    "            return (np.nan, np.nan, np.nan)\n",
    "        return (np.mean(a), np.std(a), np.sqrt(np.mean(a**2)))\n",
    "\n",
    "    fmean, fstd, frms = stats(first_mask)\n",
    "    lmean, lstd, lrms = stats(last_mask)\n",
    "    print(f\"{ch}: first mean={fmean:.4g}, std={fstd:.4g}, rms={frms:.4g} | \"\n",
    "          f\"last mean={lmean:.4g}, std={lstd:.4g}, rms={lrms:.4g}\")\n",
    "\n",
    "print(\"\\nStep 1 complete. Check 'eda_outputs/' for IEEE-quality figures.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7b7bc42-e6d7-4eb0-ac6c-591649761d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust onset detection: loading merged_dataset.csv\n",
      "Shape: (16004, 8)\n",
      "Estimated dt=1.000000e-04 s -> fs=10000.000 Hz\n",
      "Voltage cols: ['Va', 'Vb', 'Vc']\n",
      "Current cols: ['Ia', 'Ib', 'Ic']\n",
      "Computed energy (median=613.9, std=172.7, thresh=1305), derivative and short-time var (win 51 samples).\n",
      "Energy-threshold rising-edge candidates: 0\n",
      "Derivative-based peaks found: 4  (prominence used = 110827.2833850379 )\n",
      "Short-time-variance peaks found: 4  (prom_var = 116794.91909200564 )\n",
      "Total unique candidates after combining detectors: 8\n",
      "Candidates after merging by min separation: 1\n",
      "Selected events (indices): [2141]\n",
      "Corresponding times (s): [0.2141]\n",
      "Valid selected events (full window fits): 1\n",
      "\n",
      "Saved metadata to: windows_outputs_robust\\windows_metadata.csv\n",
      "Saved NPZ windows to: windows_outputs_robust\\npz\n",
      "Saved per-window plots to: windows_outputs_robust\\plots\n",
      "\n",
      "Summary:\n",
      " Total candidates considered: 1\n",
      " Total windows saved: 1\n",
      "\n",
      "First rows of metadata:\n",
      "                window_file  onset_idx  onset_time  start_idx  start_time  end_idx  end_time detected_phase                                                                         phase_scores  deriv_at_onset  energy_at_onset         score  peak_current  pre_rms_all  post_rms_all  n_samples\n",
      "window_000_onsetidx2141.npz       2141      0.2141       1941      0.1941     4141     0.014             Va {'Va': 0.22557818296916204, 'Vb': -0.006409045853165056, 'Vc': -0.11324945080974366}        5903.898       715.973761 602750.474085   19355.05251   241.120925    248.820505       2201\n",
      "Saved overview plot: windows_outputs_robust\\overview_detected_events.png\n",
      "\n",
      "Step complete. Please paste the console output here and confirm:\n",
      " - How many windows were saved?\n",
      " - Are NPZ files present in: windows_outputs_robust\\npz\n",
      " - Paste first <=8 rows from the saved metadata CSV if present.\n"
     ]
    }
   ],
   "source": [
    "# step2b_detect_onsets_robust.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import medfilt, find_peaks\n",
    "\n",
    "# === CONFIG ===\n",
    "CSV_PATH = \"merged_dataset.csv\"\n",
    "OUT_DIR = \"windows_outputs_robust\"\n",
    "PLOT_DIR = os.path.join(OUT_DIR, \"plots\")\n",
    "NPZ_DIR = os.path.join(OUT_DIR, \"npz\")\n",
    "META_CSV = os.path.join(OUT_DIR, \"windows_metadata.csv\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "os.makedirs(NPZ_DIR, exist_ok=True)\n",
    "\n",
    "PRE_SEC = 0.02    # seconds before onset\n",
    "POST_SEC = 0.2    # seconds after onset\n",
    "MAX_EVENTS = 20   # maximum windows to save (top-K if many candidates)\n",
    "MIN_EVENT_SEP_SEC = 0.05  # min separation between events when peak-picking\n",
    "\n",
    "# detection hyperparameters (you can increase sensitivity by lowering these)\n",
    "SMOOTH_KERNEL = 51   # median filter kernel (odd)\n",
    "ENERGY_STD_FACTOR = 4.0  # threshold = median + factor*std (lower -> more sensitive)\n",
    "DERIV_PROMINENCE = None  # None -> compute from derivative statistics\n",
    "VAR_WIN_MS = 5.0  # window width (ms) for short-time variance\n",
    "TOPK_FALLBACK = True  # if no peaks found, choose top-K largest derivative magnitudes\n",
    "\n",
    "print(\"Robust onset detection: loading\", CSV_PATH)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "cols = df.columns.tolist()\n",
    "tcol = next((c for c in cols if c.lower()==\"t\"), None)\n",
    "if tcol is None:\n",
    "    raise RuntimeError(\"Timestamp column 't' not found.\")\n",
    "t = df[tcol].values.astype(float)\n",
    "dt_arr = np.diff(t)\n",
    "dt_good = dt_arr[np.isfinite(dt_arr) & (dt_arr>0)]\n",
    "if len(dt_good)==0:\n",
    "    raise RuntimeError(\"Could not estimate positive time differences in 't'.\")\n",
    "dt = np.median(dt_good)\n",
    "fs = 1.0/dt\n",
    "print(f\"Estimated dt={dt:.6e} s -> fs={fs:.3f} Hz\")\n",
    "\n",
    "voltage_cols = [c for c in cols if c.lower().startswith(\"v\")]\n",
    "current_cols = [c for c in cols if c.lower().startswith(\"i\")]\n",
    "print(\"Voltage cols:\", voltage_cols)\n",
    "print(\"Current cols:\", current_cols)\n",
    "\n",
    "V = df[voltage_cols].values if len(voltage_cols)>0 else None\n",
    "I = df[current_cols].values if len(current_cols)>0 else None\n",
    "\n",
    "# --- Build signals used for detection ---\n",
    "# 1) energy proxy: sum abs voltages\n",
    "energy = np.sum(np.abs(V), axis=1) if V is not None else np.sum(np.abs(I), axis=1)\n",
    "k = SMOOTH_KERNEL if SMOOTH_KERNEL%2==1 else SMOOTH_KERNEL+1\n",
    "energy_smooth = medfilt(energy, k)\n",
    "med_e, std_e = np.median(energy_smooth), np.std(energy_smooth)\n",
    "energy_thresh = med_e + ENERGY_STD_FACTOR * std_e\n",
    "\n",
    "# 2) derivative of sum of voltages (abs derivative)\n",
    "sig_for_deriv = np.sum(V, axis=1) if V is not None else energy\n",
    "deriv = np.abs(np.concatenate(([0], np.diff(sig_for_deriv)))) / dt\n",
    "# smooth derivative with small median\n",
    "deriv_smooth = medfilt(deriv, 5)\n",
    "\n",
    "# 3) short-time variance (sliding window)\n",
    "win_len = max(1, int(round((VAR_WIN_MS/1000.0) * fs)))\n",
    "if win_len % 2 == 0: win_len += 1\n",
    "pad = win_len // 2\n",
    "arr = sig_for_deriv\n",
    "sq = arr**2\n",
    "cumsum = np.concatenate(([0], np.cumsum(sq)))\n",
    "# windowed mean of square -> variance proxy\n",
    "sts_var = (cumsum[win_len:] - cumsum[:-win_len]) / float(win_len)\n",
    "sts_var = np.concatenate((np.full(pad, sts_var[0]), sts_var, np.full(pad, sts_var[-1])))\n",
    "\n",
    "print(f\"Computed energy (median={med_e:.4g}, std={std_e:.4g}, thresh={energy_thresh:.4g}), derivative and short-time var (win {win_len} samples).\")\n",
    "\n",
    "# --- Peak picking on each detector ---\n",
    "min_dist_samples = int(round(MIN_EVENT_SEP_SEC * fs))\n",
    "candidates = set()\n",
    "\n",
    "# energy threshold rising edges\n",
    "above = energy_smooth > energy_thresh\n",
    "rising = np.nonzero(np.logical_and(above, ~np.concatenate(([False], above[:-1]))))[0]\n",
    "print(\"Energy-threshold rising-edge candidates:\", len(rising))\n",
    "for r in rising: candidates.add(int(r))\n",
    "\n",
    "# derivative peaks (prominence-based)\n",
    "if DERIV_PROMINENCE is None:\n",
    "    prom = np.median(deriv_smooth) + 3.0*np.std(deriv_smooth)\n",
    "else:\n",
    "    prom = DERIV_PROMINENCE\n",
    "peaks_deriv, prop = find_peaks(deriv_smooth, prominence=prom, distance=min_dist_samples)\n",
    "print(\"Derivative-based peaks found:\", len(peaks_deriv), \" (prominence used =\", prom, \")\")\n",
    "for p in peaks_deriv: candidates.add(int(p))\n",
    "\n",
    "# short-time variance peaks\n",
    "prom_var = np.median(sts_var) + 3.0*np.std(sts_var)\n",
    "peaks_var, _ = find_peaks(sts_var, prominence=prom_var, distance=min_dist_samples)\n",
    "print(\"Short-time-variance peaks found:\", len(peaks_var), \" (prom_var =\", prom_var, \")\")\n",
    "for p in peaks_var: candidates.add(int(p))\n",
    "\n",
    "candidates = np.array(sorted(list(candidates)))\n",
    "print(\"Total unique candidates after combining detectors:\", len(candidates))\n",
    "\n",
    "# If no candidates, fallback: choose top-K largest derivative magnitudes\n",
    "if len(candidates) == 0 and TOPK_FALLBACK:\n",
    "    K = MAX_EVENTS\n",
    "    idx_sorted = np.argsort(deriv_smooth)[-K:]\n",
    "    candidates = np.sort(idx_sorted)\n",
    "    print(f\"No peaks detected; fallback selected top-{len(candidates)} derivative indices as candidates.\")\n",
    "\n",
    "# Merge candidates to enforce min separation: pick within cluster the index with max energy jump\n",
    "if len(candidates) > 0:\n",
    "    merged = []\n",
    "    curr = candidates[0]\n",
    "    cluster = [curr]\n",
    "    for idx in candidates[1:]:\n",
    "        if idx - cluster[-1] <= min_dist_samples:\n",
    "            cluster.append(idx)\n",
    "        else:\n",
    "            # pick best representative (max derivative * energy jump)\n",
    "            cluster = np.array(cluster)\n",
    "            scores = deriv_smooth[cluster] * (energy_smooth[cluster] - med_e)\n",
    "            best = cluster[np.argmax(scores)]\n",
    "            merged.append(int(best))\n",
    "            cluster = [idx]\n",
    "    # last cluster\n",
    "    cluster = np.array(cluster)\n",
    "    scores = deriv_smooth[cluster] * (energy_smooth[cluster] - med_e)\n",
    "    best = cluster[np.argmax(scores)]\n",
    "    merged.append(int(best))\n",
    "    candidates = np.array(merged)\n",
    "    print(\"Candidates after merging by min separation:\", len(candidates))\n",
    "\n",
    "# Score candidates and take top-K by score\n",
    "scores = (deriv_smooth[candidates] * (energy_smooth[candidates] - med_e))\n",
    "order = np.argsort(scores)[::-1]\n",
    "selected = candidates[order][:MAX_EVENTS]\n",
    "selected = np.sort(selected)\n",
    "print(\"Selected events (indices):\", selected.tolist())\n",
    "print(\"Corresponding times (s):\", [float(t[int(i)]) for i in selected])\n",
    "\n",
    "# Filter out those too close to edges for window extraction\n",
    "pre_samples = int(round(PRE_SEC * fs))\n",
    "post_samples = int(round(POST_SEC * fs))\n",
    "valid_selected = [int(i) for i in selected if i - pre_samples >= 0 and i + post_samples < len(df)]\n",
    "print(\"Valid selected events (full window fits):\", len(valid_selected))\n",
    "\n",
    "# Extract windows and save\n",
    "rows = []\n",
    "for j, onset_idx in enumerate(valid_selected):\n",
    "    start_idx = onset_idx - pre_samples\n",
    "    end_idx = onset_idx + post_samples\n",
    "    t_window = t[start_idx:end_idx+1]\n",
    "    V_window = V[start_idx:end_idx+1] if V is not None else None\n",
    "    I_window = I[start_idx:end_idx+1] if I is not None else None\n",
    "\n",
    "    # heuristic phase detection (relative RMS drop)\n",
    "    phase_detect = None\n",
    "    phase_scores = {}\n",
    "    if V_window is not None:\n",
    "        pre_seg = V_window[:pre_samples]\n",
    "        post_seg = V_window[pre_samples:pre_samples + max(1, int(round(0.05*fs)))]\n",
    "        for pi, ch in enumerate(voltage_cols):\n",
    "            pre_rms = np.sqrt(np.mean(pre_seg[:,pi]**2)) if len(pre_seg)>0 else 0.0\n",
    "            post_rms = np.sqrt(np.mean(post_seg[:,pi]**2)) if len(post_seg)>0 else 0.0\n",
    "            score = (pre_rms - post_rms) / (pre_rms + 1e-12)\n",
    "            phase_scores[ch] = float(score)\n",
    "        phase_detect = max(phase_scores.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "    # stats\n",
    "    peak_curr = float(np.max(np.abs(I_window))) if I_window is not None else np.nan\n",
    "    pre_rms_all = float(np.sqrt(np.mean(V_window[:pre_samples,:]**2))) if V_window is not None else np.nan\n",
    "    post_rms_all = float(np.sqrt(np.mean(V_window[pre_samples:pre_samples+max(1,int(round(0.05*fs))),:]**2))) if V_window is not None else np.nan\n",
    "\n",
    "    fname = f\"window_{j:03d}_onsetidx{onset_idx}.npz\"\n",
    "    fpath = os.path.join(NPZ_DIR, fname)\n",
    "    np.savez_compressed(fpath, t=t_window, V=V_window, I=I_window, onset_idx=int(onset_idx),\n",
    "                        start_idx=int(start_idx), end_idx=int(end_idx))\n",
    "    rows.append({\n",
    "        \"window_file\": fname,\n",
    "        \"onset_idx\": int(onset_idx),\n",
    "        \"onset_time\": float(t[onset_idx]),\n",
    "        \"start_idx\": int(start_idx),\n",
    "        \"start_time\": float(t[start_idx]),\n",
    "        \"end_idx\": int(end_idx),\n",
    "        \"end_time\": float(t[end_idx]),\n",
    "        \"detected_phase\": phase_detect,\n",
    "        \"phase_scores\": str(phase_scores),\n",
    "        \"deriv_at_onset\": float(deriv_smooth[onset_idx]),\n",
    "        \"energy_at_onset\": float(energy_smooth[onset_idx]),\n",
    "        \"score\": float(scores[np.where(candidates==onset_idx)[0][0]]) if onset_idx in candidates else float(deriv_smooth[onset_idx]),\n",
    "        \"peak_current\": peak_curr,\n",
    "        \"pre_rms_all\": pre_rms_all,\n",
    "        \"post_rms_all\": post_rms_all,\n",
    "        \"n_samples\": int(end_idx - start_idx + 1)\n",
    "    })\n",
    "\n",
    "    # save per-window plot\n",
    "    fig, axs = plt.subplots(2,1, figsize=(10,5), sharex=True)\n",
    "    if V_window is not None:\n",
    "        for pi, ch in enumerate(voltage_cols):\n",
    "            axs[0].plot(t_window - t_window[0], V_window[:,pi], label=ch, linewidth=0.8)\n",
    "        axs[0].legend(fontsize='small')\n",
    "    if I_window is not None:\n",
    "        for pi, ch in enumerate(current_cols):\n",
    "            axs[1].plot(t_window - t_window[0], I_window[:,pi], label=ch, linewidth=0.8)\n",
    "        axs[1].legend(fontsize='small')\n",
    "    axs[0].axvline(x=pre_samples*dt, color='k', linestyle='--')\n",
    "    axs[1].axvline(x=pre_samples*dt, color='k', linestyle='--')\n",
    "    axs[1].set_xlabel(\"time (s) rel window start\")\n",
    "    axs[0].set_title(f\"Window {j:03d} onset@{t[onset_idx]:.6f}s detected_phase={phase_detect}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOT_DIR, f\"window_{j:03d}.png\"), dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "# Save metadata\n",
    "meta_df = pd.DataFrame(rows)\n",
    "meta_df.to_csv(META_CSV, index=False)\n",
    "print(\"\\nSaved metadata to:\", META_CSV)\n",
    "print(\"Saved NPZ windows to:\", NPZ_DIR)\n",
    "print(\"Saved per-window plots to:\", PLOT_DIR)\n",
    "print(\"\\nSummary:\")\n",
    "print(\" Total candidates considered:\", len(candidates))\n",
    "print(\" Total windows saved:\", len(meta_df))\n",
    "if len(meta_df)>0:\n",
    "    print(\"\\nFirst rows of metadata:\")\n",
    "    with pd.option_context('display.max_colwidth', 200):\n",
    "        print(meta_df.head(8).to_string(index=False))\n",
    "\n",
    "# Also save an overview plot with detected events marked\n",
    "fig, ax = plt.subplots(1,1, figsize=(14,4))\n",
    "ax.plot(t, energy_smooth, linewidth=0.6, label='energy_smooth (sum|V|)')\n",
    "ax.axhline(energy_thresh, color='r', linestyle='--', label='energy_thresh')\n",
    "for onset_idx in selected:\n",
    "    ax.axvline(t[onset_idx], color='k', alpha=0.6)\n",
    "ax.set_xlabel(\"t (s)\")\n",
    "ax.set_ylabel(\"energy proxy\")\n",
    "ax.set_title(\"Overview: detected events marked\")\n",
    "ax.legend(fontsize='small')\n",
    "plt.tight_layout()\n",
    "overview_png = os.path.join(OUT_DIR, \"overview_detected_events.png\")\n",
    "plt.savefig(overview_png, dpi=150)\n",
    "plt.close(fig)\n",
    "print(\"Saved overview plot:\", overview_png)\n",
    "\n",
    "print(\"\\nStep complete. Please paste the console output here and confirm:\")\n",
    "print(\" - How many windows were saved?\")\n",
    "print(\" - Are NPZ files present in:\", NPZ_DIR)\n",
    "print(\" - Paste first <=8 rows from the saved metadata CSV if present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd9ebdc4-1d58-437b-98f1-a6805bc42e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust onset detection: loading merged_dataset.csv\n",
      "Shape: (16004, 8)\n",
      "Estimated dt=1.000000e-04 s -> fs=10000.000 Hz\n",
      "Voltage cols: ['Va', 'Vb', 'Vc']\n",
      "Current cols: ['Ia', 'Ib', 'Ic']\n",
      "Computed energy (median=613.9, std=172.7, thresh=1305).\n",
      "Saved overview plot: windows_outputs_robust\\overview_detected_events.png\n"
     ]
    }
   ],
   "source": [
    "# step2b_detect_onsets_robust_ieee.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import medfilt, find_peaks\n",
    "\n",
    "# === CONFIG ===\n",
    "CSV_PATH = \"merged_dataset.csv\"\n",
    "OUT_DIR = \"windows_outputs_robust\"\n",
    "PLOT_DIR = os.path.join(OUT_DIR, \"plots\")\n",
    "NPZ_DIR = os.path.join(OUT_DIR, \"npz\")\n",
    "META_CSV = os.path.join(OUT_DIR, \"windows_metadata.csv\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "os.makedirs(NPZ_DIR, exist_ok=True)\n",
    "\n",
    "# === IEEE Publication Plot Settings ===\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times New Roman\", \"Times\", \"DejaVu Serif\", \"Nimbus Roman No9 L\"],\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelweight\": \"bold\",\n",
    "    \"axes.titleweight\": \"bold\",\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"legend.fontsize\": 11,\n",
    "    \"figure.dpi\": 120,\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"savefig.bbox\": \"tight\",\n",
    "    \"savefig.pad_inches\": 0.05,\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \"grid.linestyle\": \"--\"\n",
    "})\n",
    "\n",
    "# === PARAMETERS ===\n",
    "PRE_SEC = 0.02\n",
    "POST_SEC = 0.2\n",
    "MAX_EVENTS = 20\n",
    "MIN_EVENT_SEP_SEC = 0.05\n",
    "SMOOTH_KERNEL = 51\n",
    "ENERGY_STD_FACTOR = 4.0\n",
    "DERIV_PROMINENCE = None\n",
    "VAR_WIN_MS = 5.0\n",
    "TOPK_FALLBACK = True\n",
    "\n",
    "print(\"Robust onset detection: loading\", CSV_PATH)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "\n",
    "cols = df.columns.tolist()\n",
    "tcol = next((c for c in cols if c.lower() == \"t\"), None)\n",
    "if tcol is None:\n",
    "    raise RuntimeError(\"Timestamp column 't' not found.\")\n",
    "t = df[tcol].values.astype(float)\n",
    "dt_arr = np.diff(t)\n",
    "dt_good = dt_arr[np.isfinite(dt_arr) & (dt_arr > 0)]\n",
    "if len(dt_good) == 0:\n",
    "    raise RuntimeError(\"Could not estimate positive time differences in 't'.\")\n",
    "dt = np.median(dt_good)\n",
    "fs = 1.0 / dt\n",
    "print(f\"Estimated dt={dt:.6e} s -> fs={fs:.3f} Hz\")\n",
    "\n",
    "voltage_cols = [c for c in cols if c.lower().startswith(\"v\")]\n",
    "current_cols = [c for c in cols if c.lower().startswith(\"i\")]\n",
    "print(\"Voltage cols:\", voltage_cols)\n",
    "print(\"Current cols:\", current_cols)\n",
    "\n",
    "V = df[voltage_cols].values if len(voltage_cols) > 0 else None\n",
    "I = df[current_cols].values if len(current_cols) > 0 else None\n",
    "\n",
    "# --- Energy and features ---\n",
    "energy = np.sum(np.abs(V), axis=1) if V is not None else np.sum(np.abs(I), axis=1)\n",
    "k = SMOOTH_KERNEL if SMOOTH_KERNEL % 2 == 1 else SMOOTH_KERNEL + 1\n",
    "energy_smooth = medfilt(energy, k)\n",
    "med_e, std_e = np.median(energy_smooth), np.std(energy_smooth)\n",
    "energy_thresh = med_e + ENERGY_STD_FACTOR * std_e\n",
    "sig_for_deriv = np.sum(V, axis=1) if V is not None else energy\n",
    "deriv = np.abs(np.concatenate(([0], np.diff(sig_for_deriv)))) / dt\n",
    "deriv_smooth = medfilt(deriv, 5)\n",
    "\n",
    "# short-time variance\n",
    "win_len = max(1, int(round((VAR_WIN_MS / 1000.0) * fs)))\n",
    "if win_len % 2 == 0: win_len += 1\n",
    "pad = win_len // 2\n",
    "arr = sig_for_deriv\n",
    "sq = arr ** 2\n",
    "cumsum = np.concatenate(([0], np.cumsum(sq)))\n",
    "sts_var = (cumsum[win_len:] - cumsum[:-win_len]) / float(win_len)\n",
    "sts_var = np.concatenate((np.full(pad, sts_var[0]), sts_var, np.full(pad, sts_var[-1])))\n",
    "\n",
    "print(f\"Computed energy (median={med_e:.4g}, std={std_e:.4g}, thresh={energy_thresh:.4g}).\")\n",
    "\n",
    "# --- Peak detection ---\n",
    "min_dist_samples = int(round(MIN_EVENT_SEP_SEC * fs))\n",
    "candidates = set()\n",
    "\n",
    "above = energy_smooth > energy_thresh\n",
    "rising = np.nonzero(np.logical_and(above, ~np.concatenate(([False], above[:-1]))))[0]\n",
    "for r in rising: candidates.add(int(r))\n",
    "\n",
    "if DERIV_PROMINENCE is None:\n",
    "    prom = np.median(deriv_smooth) + 3.0 * np.std(deriv_smooth)\n",
    "else:\n",
    "    prom = DERIV_PROMINENCE\n",
    "peaks_deriv, _ = find_peaks(deriv_smooth, prominence=prom, distance=min_dist_samples)\n",
    "for p in peaks_deriv: candidates.add(int(p))\n",
    "\n",
    "prom_var = np.median(sts_var) + 3.0 * np.std(sts_var)\n",
    "peaks_var, _ = find_peaks(sts_var, prominence=prom_var, distance=min_dist_samples)\n",
    "for p in peaks_var: candidates.add(int(p))\n",
    "\n",
    "candidates = np.array(sorted(list(candidates)))\n",
    "if len(candidates) == 0 and TOPK_FALLBACK:\n",
    "    K = MAX_EVENTS\n",
    "    idx_sorted = np.argsort(deriv_smooth)[-K:]\n",
    "    candidates = np.sort(idx_sorted)\n",
    "\n",
    "# Merge nearby events\n",
    "if len(candidates) > 0:\n",
    "    merged = []\n",
    "    curr = candidates[0]\n",
    "    cluster = [curr]\n",
    "    for idx in candidates[1:]:\n",
    "        if idx - cluster[-1] <= min_dist_samples:\n",
    "            cluster.append(idx)\n",
    "        else:\n",
    "            cluster = np.array(cluster)\n",
    "            scores = deriv_smooth[cluster] * (energy_smooth[cluster] - med_e)\n",
    "            best = cluster[np.argmax(scores)]\n",
    "            merged.append(int(best))\n",
    "            cluster = [idx]\n",
    "    cluster = np.array(cluster)\n",
    "    scores = deriv_smooth[cluster] * (energy_smooth[cluster] - med_e)\n",
    "    best = cluster[np.argmax(scores)]\n",
    "    merged.append(int(best))\n",
    "    candidates = np.array(merged)\n",
    "\n",
    "# Select top events\n",
    "scores = (deriv_smooth[candidates] * (energy_smooth[candidates] - med_e))\n",
    "order = np.argsort(scores)[::-1]\n",
    "selected = candidates[order][:MAX_EVENTS]\n",
    "selected = np.sort(selected)\n",
    "\n",
    "pre_samples = int(round(PRE_SEC * fs))\n",
    "post_samples = int(round(POST_SEC * fs))\n",
    "valid_selected = [int(i) for i in selected if i - pre_samples >= 0 and i + post_samples < len(df)]\n",
    "\n",
    "rows = []\n",
    "for j, onset_idx in enumerate(valid_selected):\n",
    "    start_idx = onset_idx - pre_samples\n",
    "    end_idx = onset_idx + post_samples\n",
    "    t_window = t[start_idx:end_idx + 1]\n",
    "    V_window = V[start_idx:end_idx + 1] if V is not None else None\n",
    "    I_window = I[start_idx:end_idx + 1] if I is not None else None\n",
    "\n",
    "    # phase detection\n",
    "    phase_detect = None\n",
    "    phase_scores = {}\n",
    "    if V_window is not None:\n",
    "        pre_seg = V_window[:pre_samples]\n",
    "        post_seg = V_window[pre_samples:pre_samples + max(1, int(round(0.05 * fs)))]\n",
    "        for pi, ch in enumerate(voltage_cols):\n",
    "            pre_rms = np.sqrt(np.mean(pre_seg[:, pi] ** 2)) if len(pre_seg) > 0 else 0.0\n",
    "            post_rms = np.sqrt(np.mean(post_seg[:, pi] ** 2)) if len(post_seg) > 0 else 0.0\n",
    "            score = (pre_rms - post_rms) / (pre_rms + 1e-12)\n",
    "            phase_scores[ch] = float(score)\n",
    "        phase_detect = max(phase_scores.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "    # per-window plot === IEEE Style ===\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 5), sharex=True)\n",
    "    if V_window is not None:\n",
    "        for pi, ch in enumerate(voltage_cols):\n",
    "            axs[0].plot(t_window - t_window[0], V_window[:, pi], label=ch, linewidth=0.8)\n",
    "        axs[0].set_ylabel(\"V (Volts)\", fontweight='bold')\n",
    "        axs[0].legend(loc='upper right')\n",
    "    if I_window is not None:\n",
    "        for pi, ch in enumerate(current_cols):\n",
    "            axs[1].plot(t_window - t_window[0], I_window[:, pi], label=ch, linewidth=0.8)\n",
    "        axs[1].set_ylabel(\"I (Amperes)\", fontweight='bold')\n",
    "        axs[1].legend(loc='upper right')\n",
    "    axs[1].set_xlabel(\"Time (s)\", fontweight='bold')\n",
    "    axs[0].axvline(x=pre_samples * dt, color='k', linestyle='--')\n",
    "    axs[1].axvline(x=pre_samples * dt, color='k', linestyle='--')\n",
    "    axs[0].set_title(f\"Window {j:03d} | Onset @ {t[onset_idx]:.6f}s | Phase = {phase_detect}\", fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    figpath = os.path.join(PLOT_DIR, f\"window_{j:03d}.png\")\n",
    "    plt.savefig(figpath, format=\"png\")\n",
    "    plt.savefig(figpath.replace(\".png\", \".pdf\"), format=\"pdf\")\n",
    "    plt.close(fig)\n",
    "\n",
    "# --- Overview Plot (IEEE Style) ---\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "ax.plot(t, energy_smooth, linewidth=0.8, label='Energy (sum|V|)')\n",
    "ax.axhline(energy_thresh, color='r', linestyle='--', label='Threshold')\n",
    "for onset_idx in selected:\n",
    "    ax.axvline(t[onset_idx], color='k', linestyle='--', alpha=0.6)\n",
    "ax.set_xlabel(\"Time (s)\", fontweight='bold')\n",
    "ax.set_ylabel(\"Energy Proxy (a.u.)\", fontweight='bold')\n",
    "ax.set_title(\"Detected Event Onsets\", fontweight='bold')\n",
    "ax.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "overview_png = os.path.join(OUT_DIR, \"overview_detected_events.png\")\n",
    "plt.savefig(overview_png, format=\"png\")\n",
    "plt.savefig(overview_png.replace(\".png\", \".pdf\"), format=\"pdf\")\n",
    "plt.close(fig)\n",
    "print(\"Saved overview plot:\", overview_png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "402a5659-5333-4766-863f-f12c2597b86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating combined summary plot for all detected windows...\n",
      "Saved combined summary figure: windows_outputs_robust\\plots\\combined_summary_windows.png\n"
     ]
    }
   ],
   "source": [
    "# === Combined Summary Plot (all windows stacked vertically) ===\n",
    "if len(valid_selected) > 0:\n",
    "    print(\"Creating combined summary plot for all detected windows...\")\n",
    "\n",
    "    # Determine time axis length (all windows same size)\n",
    "    window_len = pre_samples + post_samples + 1\n",
    "    time_axis = np.linspace(-PRE_SEC, POST_SEC, window_len)\n",
    "\n",
    "    # Prepare figure\n",
    "    n_windows = len(valid_selected)\n",
    "    fig, axs = plt.subplots(n_windows, 1, figsize=(10, 2.2 * n_windows), sharex=True)\n",
    "\n",
    "    if n_windows == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    for j, (ax, onset_idx) in enumerate(zip(axs, valid_selected)):\n",
    "        start_idx = onset_idx - pre_samples\n",
    "        end_idx = onset_idx + post_samples\n",
    "        V_window = V[start_idx:end_idx + 1] if V is not None else None\n",
    "        I_window = I[start_idx:end_idx + 1] if I is not None else None\n",
    "\n",
    "        # Voltage or Current selection for clarity\n",
    "        if V_window is not None:\n",
    "            for pi, ch in enumerate(voltage_cols):\n",
    "                ax.plot(time_axis, V_window[:, pi], label=ch, linewidth=0.8)\n",
    "            ax.set_ylabel(\"V (Volts)\", fontweight=\"bold\")\n",
    "        elif I_window is not None:\n",
    "            for pi, ch in enumerate(current_cols):\n",
    "                ax.plot(time_axis, I_window[:, pi], label=ch, linewidth=0.8)\n",
    "            ax.set_ylabel(\"I (Amperes)\", fontweight=\"bold\")\n",
    "\n",
    "        # Mark the onset\n",
    "        ax.axvline(0.0, color=\"k\", linestyle=\"--\", linewidth=1)\n",
    "        ax.grid(True, linewidth=0.3)\n",
    "        ax.set_title(f\"Event {j+1}: Onset @ {t[onset_idx]:.6f} s\", fontweight=\"bold\", loc=\"left\", fontsize=12)\n",
    "\n",
    "        if j == 0:\n",
    "            ax.legend(loc=\"upper right\")\n",
    "\n",
    "    axs[-1].set_xlabel(\"Time (s)\", fontweight=\"bold\")\n",
    "\n",
    "    fig.suptitle(\"Combined Summary of Detected Event Windows\", fontweight=\"bold\", fontsize=14)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "\n",
    "    summary_path = os.path.join(PLOT_DIR, \"combined_summary_windows.png\")\n",
    "    plt.savefig(summary_path, format=\"png\")\n",
    "    plt.savefig(summary_path.replace(\".png\", \".pdf\"), format=\"pdf\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(\"Saved combined summary figure:\", summary_path)\n",
    "else:\n",
    "    print(\"No valid detected windows available for summary plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbaa9d7f-445d-4e8b-8dfd-ec0f3ad80c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NPZ folder: windows_outputs_robust/npz\n",
      "Total NPZ files: 1\n",
      " Loaded metadata from: windows_outputs_robust\\windows_metadata.csv\n",
      "\n",
      "Metadata columns: ['window_file', 'onset_idx', 'onset_time', 'start_idx', 'start_time', 'end_idx', 'end_time', 'detected_phase', 'phase_scores', 'deriv_at_onset', 'energy_at_onset', 'score', 'peak_current', 'pre_rms_all', 'post_rms_all', 'n_samples']\n",
      "Total metadata rows: 1\n",
      "\n",
      "Detected phase distribution:\n",
      "detected_phase\n",
      "Va    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Example file: windows_outputs_robust/npz\\window_000_onsetidx2141.npz\n",
      "  t: shape=(2201,), dtype=float64\n",
      "  V: shape=(2201, 3), dtype=float64\n",
      "  I: shape=(2201, 3), dtype=float64\n",
      "  onset_idx: shape=(), dtype=int32\n",
      "  start_idx: shape=(), dtype=int32\n",
      "  end_idx: shape=(), dtype=int32\n"
     ]
    }
   ],
   "source": [
    "# diagnose_windows_dataset.py\n",
    "import os, glob, numpy as np, pandas as pd\n",
    "\n",
    "paths = [\n",
    "    \"windows_outputs_robust/npz\",\n",
    "    \"windows_outputs/npz\",\n",
    "    \"windows_outputs_robust\",\n",
    "    \"windows_outputs\"\n",
    "]\n",
    "npz_folder = None\n",
    "for p in paths:\n",
    "    if os.path.isdir(p):\n",
    "        files = glob.glob(os.path.join(p, \"*.npz\"))\n",
    "        if len(files) > 0:\n",
    "            npz_folder = p\n",
    "            break\n",
    "if npz_folder is None:\n",
    "    print(\" No .npz found  please rerun window extraction.\")\n",
    "else:\n",
    "    print(\" NPZ folder:\", npz_folder)\n",
    "    files = sorted(glob.glob(os.path.join(npz_folder, \"*.npz\")))\n",
    "    print(\"Total NPZ files:\", len(files))\n",
    "\n",
    "meta_candidates = [\n",
    "    os.path.join(os.path.dirname(npz_folder), \"windows_metadata.csv\"),\n",
    "    os.path.join(\".\", \"windows_metadata.csv\"),\n",
    "]\n",
    "meta = None\n",
    "for m in meta_candidates:\n",
    "    if os.path.isfile(m):\n",
    "        try:\n",
    "            meta = pd.read_csv(m)\n",
    "            print(\" Loaded metadata from:\", m)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Failed to load metadata\", m, \"->\", e)\n",
    "            meta = None\n",
    "\n",
    "if meta is not None:\n",
    "    print(\"\\nMetadata columns:\", meta.columns.tolist())\n",
    "    print(\"Total metadata rows:\", len(meta))\n",
    "    if \"detected_phase\" in meta.columns:\n",
    "        print(\"\\nDetected phase distribution:\")\n",
    "        print(meta[\"detected_phase\"].value_counts(dropna=False))\n",
    "    else:\n",
    "        print(\" 'detected_phase' column not found.\")\n",
    "else:\n",
    "    print(\" No metadata CSV found near the npz folder.\")\n",
    "\n",
    "# Try to load one example .npz\n",
    "if npz_folder is not None and len(files)>0:\n",
    "    z = np.load(files[0], allow_pickle=True)\n",
    "    print(\"\\nExample file:\", files[0])\n",
    "    for k in z.keys():\n",
    "        arr = z[k]\n",
    "        if hasattr(arr, \"shape\"):\n",
    "            print(f\"  {k}: shape={arr.shape}, dtype={arr.dtype}\")\n",
    "        else:\n",
    "            print(f\"  {k}: {arr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b213887-aba4-48e0-9832-1e64f180e2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found NPZ files: ['windows_outputs_robust/npz\\\\window_000_onsetidx2141.npz']\n",
      "Original window shape (T,C): (2201, 3)\n",
      "Estimated fs: 10000.0000000011\n",
      "Total augmented samples created: 401\n",
      "Feature matrix shape: (401, 52)\n",
      "Label distribution: {0: 401}\n",
      "Saved augmented features to ml_aug_out\\augmented_features.csv\n",
      "Warning: only one class present (Va). Proceeding to split without stratify.\n",
      "Train/val/test sizes: 280 60 61\n",
      "\n",
      "Only one class available (Va). Training classifiers requires >=2 classes.\n",
      "Saved augmented features; you can either (A) create synthetic examples for other phases, (B) re-run detection to get more events, or (C) perform unsupervised analysis.\n",
      "Saved artifacts to ml_aug_out\n",
      "Step complete. If you want, I can now:\n",
      "  1) generate synthetic B/C windows by perturbing phase relationships (needs rule-based approach),\n",
      "  2) re-run detection more aggressively across the entire raw CSV to try to find more events, or\n",
      "  3) show unsupervised clustering / anomaly-detection using the augmented set.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "# step3_aug_and_train_ml.py\n",
    "import os, glob, math, random, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np, pandas as pd\n",
    "from scipy import stats, signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Optional models\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except Exception:\n",
    "    xgb = None\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception:\n",
    "    lgb = None\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "except Exception:\n",
    "    CatBoostClassifier = None\n",
    "\n",
    "# === CONFIG ===\n",
    "NPZ_FOLDER = \"windows_outputs_robust/npz\"\n",
    "OUT_DIR = \"ml_aug_out\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "N_AUG = 400           # number of augmented samples to create (choose 200-1000)\n",
    "KEEP_ORIGINAL = True  # include the original window as one sample\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE = 0.15\n",
    "CV_FOLDS = 5\n",
    "\n",
    "# augmentation hyperparams\n",
    "NOISE_STD_REL = 0.02   # noise std as fraction of signal RMS\n",
    "SCALE_MIN, SCALE_MAX = 0.8, 1.2\n",
    "TIME_SHIFT_MAX_SAMPLES = 50  # shift left/right by up to this many samples (zero-pad)\n",
    "TIME_STRETCH_MIN, TIME_STRETCH_MAX = 0.9, 1.1  # resample factor\n",
    "CROP_MIN_FRAC = 0.8   # random crop keeps between 80% and 100% of length, then pad back\n",
    "\n",
    "# feature helpers (same as before)\n",
    "from scipy.signal import welch\n",
    "\n",
    "def time_features(ch):\n",
    "    f = {}\n",
    "    f['mean'] = np.mean(ch)\n",
    "    f['std'] = np.std(ch)\n",
    "    f['rms'] = np.sqrt(np.mean(ch**2))\n",
    "    f['ptp'] = np.ptp(ch)\n",
    "    f['abs_mean'] = np.mean(np.abs(ch))\n",
    "    f['skew'] = float(stats.skew(ch))\n",
    "    f['kurtosis'] = float(stats.kurtosis(ch))\n",
    "    f['median'] = np.median(ch)\n",
    "    f['max'] = np.max(ch)\n",
    "    f['min'] = np.min(ch)\n",
    "    f['energy'] = np.sum(ch**2)\n",
    "    return f\n",
    "\n",
    "def spectral_features(ch, fs):\n",
    "    try:\n",
    "        f, Pxx = welch(ch, fs=fs, nperseg=min(256, len(ch)))\n",
    "        sc = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12)\n",
    "        dom = f[np.argmax(Pxx)]\n",
    "        nyq = fs / 2.0\n",
    "        low = np.sum(Pxx[(f >= 0) & (f < 0.1*nyq)])\n",
    "        mid = np.sum(Pxx[(f >= 0.1*nyq) & (f < 0.4*nyq)])\n",
    "        high = np.sum(Pxx[(f >= 0.4*nyq)])\n",
    "        return {'spec_centroid': sc, 'spec_domfreq': dom, 'spec_low_energy': low, 'spec_mid_energy': mid, 'spec_high_energy': high}\n",
    "    except Exception:\n",
    "        return {'spec_centroid': np.nan, 'spec_domfreq': np.nan, 'spec_low_energy': np.nan, 'spec_mid_energy': np.nan, 'spec_high_energy': np.nan}\n",
    "\n",
    "# augmentation transforms\n",
    "def add_noise(x, rel_std):\n",
    "    # x: T array\n",
    "    rms = np.sqrt(np.mean(x**2))\n",
    "    sigma = rel_std * (rms + 1e-12)\n",
    "    return x + np.random.normal(0, sigma, size=x.shape)\n",
    "\n",
    "def scale_amplitude(x, low=SCALE_MIN, high=SCALE_MAX):\n",
    "    factor = np.random.uniform(low, high)\n",
    "    return x * factor\n",
    "\n",
    "def time_shift(x, max_shift):\n",
    "    s = np.random.randint(-max_shift, max_shift+1)\n",
    "    if s == 0:\n",
    "        return x\n",
    "    if s > 0:\n",
    "        # shift right => pad left\n",
    "        return np.pad(x[:-s], (s,0), mode='constant')\n",
    "    else:\n",
    "        s = abs(s)\n",
    "        return np.pad(x[s:], (0,s), mode='constant')\n",
    "\n",
    "def time_stretch_resample(x, factor):\n",
    "    # simple resample using Fourier/resample\n",
    "    if abs(factor - 1.0) < 1e-6:\n",
    "        return x\n",
    "    new_len = max(2, int(round(len(x) * factor)))\n",
    "    return signal.resample(x, new_len)\n",
    "\n",
    "def random_crop_and_pad(x, min_frac=CROP_MIN_FRAC):\n",
    "    L = len(x)\n",
    "    keep = int(np.round(np.random.uniform(min_frac, 1.0) * L))\n",
    "    if keep >= L:\n",
    "        return x\n",
    "    start = np.random.randint(0, L - keep + 1)\n",
    "    cropped = x[start:start+keep]\n",
    "    # pad back to L with zeros\n",
    "    pad_left = (L - keep)//2\n",
    "    pad_right = L - keep - pad_left\n",
    "    return np.pad(cropped, (pad_left, pad_right), mode='constant')\n",
    "\n",
    "def small_filter(x, fs):\n",
    "    # randomly apply a mild lowpass or highpass via FIR\n",
    "    if np.random.rand() < 0.5:\n",
    "        # lowpass: cutoff between 0.1*nyq and 0.4*nyq\n",
    "        nyq = 0.5 * fs\n",
    "        cutoff = np.random.uniform(0.05*nyq, 0.4*nyq)\n",
    "        # design simple FIR with window\n",
    "        from scipy.signal import firwin, lfilter\n",
    "        taps = firwin(numtaps=31, cutoff=cutoff/(fs/2.0))\n",
    "        return lfilter(taps, [1.0], x)\n",
    "    else:\n",
    "        # highpass small\n",
    "        nyq = 0.5 * fs\n",
    "        cutoff = np.random.uniform(0.02*nyq, 0.1*nyq)\n",
    "        from scipy.signal import firwin, lfilter\n",
    "        taps = firwin(numtaps=31, cutoff=cutoff/(fs/2.0), pass_zero=False)\n",
    "        return lfilter(taps, [1.0], x)\n",
    "\n",
    "# find the single npz window\n",
    "files = sorted(glob.glob(os.path.join(NPZ_FOLDER, \"*.npz\")))\n",
    "if len(files) == 0:\n",
    "    raise SystemExit(\"No NPZ found in \" + NPZ_FOLDER)\n",
    "print(\"Found NPZ files:\", files)\n",
    "# pick first one (we know there is one)\n",
    "z = np.load(files[0], allow_pickle=True)\n",
    "V = z.get(\"V\", None)\n",
    "I = z.get(\"I\", None)\n",
    "t = z.get(\"t\", None)\n",
    "if V is None:\n",
    "    raise SystemExit(\"No V in the NPZ.\")\n",
    "V = np.asarray(V)  # shape T x C\n",
    "if V.ndim == 2 and V.shape[0] < V.shape[1]:\n",
    "    V = V.T\n",
    "T, C = V.shape\n",
    "print(\"Original window shape (T,C):\", V.shape)\n",
    "if t is not None and len(t)>2:\n",
    "    dt = np.median(np.diff(t.astype(float)))\n",
    "    fs = 1.0/dt\n",
    "else:\n",
    "    fs = 10000.0\n",
    "print(\"Estimated fs:\", fs)\n",
    "\n",
    "# create augmentation set\n",
    "augmented = []\n",
    "labels = []\n",
    "filenames = []\n",
    "\n",
    "# include original\n",
    "if KEEP_ORIGINAL:\n",
    "    augmented.append(V.copy())\n",
    "    filenames.append(os.path.basename(files[0]) + \"_orig\")\n",
    "    labels.append(\"Va\")  # keep same detected_phase label\n",
    "\n",
    "# generate N_AUG augmented examples by applying random chains of transforms\n",
    "for i in range(N_AUG):\n",
    "    V_aug = V.copy()  # T x C\n",
    "    # for each channel apply slightly different transforms\n",
    "    V_new = np.zeros_like(V_aug)\n",
    "    for ch in range(C):\n",
    "        ch_signal = V_aug[:, ch]\n",
    "        # random sequence of transforms\n",
    "        # 1) time stretch occasionally\n",
    "        if np.random.rand() < 0.15:\n",
    "            factor = np.random.uniform(TIME_STRETCH_MIN, TIME_STRETCH_MAX)\n",
    "            s = time_stretch_resample(ch_signal, factor)\n",
    "            # resample back to original length by trunc/pad or resample\n",
    "            if len(s) != T:\n",
    "                s = signal.resample(s, T)\n",
    "            ch_signal = s\n",
    "        # 2) crop and pad sometimes\n",
    "        if np.random.rand() < 0.2:\n",
    "            ch_signal = random_crop_and_pad(ch_signal, min_frac=CROP_MIN_FRAC)\n",
    "        # 3) time shift\n",
    "        if np.random.rand() < 0.4:\n",
    "            ch_signal = time_shift(ch_signal, TIME_SHIFT_MAX_SAMPLES)\n",
    "        # 4) amplitude scale\n",
    "        if np.random.rand() < 0.6:\n",
    "            ch_signal = scale_amplitude(ch_signal, SCALE_MIN, SCALE_MAX)\n",
    "        # 5) filter\n",
    "        if np.random.rand() < 0.3:\n",
    "            ch_signal = small_filter(ch_signal, fs)\n",
    "        # 6) additive noise\n",
    "        if np.random.rand() < 0.9:\n",
    "            ch_signal = add_noise(ch_signal, NOISE_STD_REL)\n",
    "        V_new[:, ch] = ch_signal\n",
    "    augmented.append(V_new)\n",
    "    filenames.append(os.path.basename(files[0]) + f\"_aug{i:04d}\")\n",
    "    labels.append(\"Va\")\n",
    "\n",
    "print(\"Total augmented samples created:\", len(augmented))\n",
    "\n",
    "# compute features for each augmented sample\n",
    "rows = []\n",
    "for idx, Vx in enumerate(augmented):\n",
    "    feat = {}\n",
    "    # ensure shape T x C\n",
    "    if Vx.ndim == 1:\n",
    "        Vx = Vx.reshape(-1,1)\n",
    "    if Vx.shape[0] != T:\n",
    "        # resample to T\n",
    "        Vx = signal.resample(Vx, T)\n",
    "    for ch in range(Vx.shape[1]):\n",
    "        arr = Vx[:, ch].astype(float)\n",
    "        tfs = time_features(arr)\n",
    "        sfs = spectral_features(arr, fs)\n",
    "        prefix = f\"V{ch+1}\"\n",
    "        for k, v in tfs.items():\n",
    "            feat[f\"{prefix}_{k}\"] = v\n",
    "        for k, v in sfs.items():\n",
    "            feat[f\"{prefix}_{k}\"] = v\n",
    "    # cross-channel RMS stats if >=3 channels\n",
    "    if Vx.shape[1] >= 3:\n",
    "        rms_vals = [np.sqrt(np.mean(Vx[:,c]**2)) for c in range(3)]\n",
    "        feat['V_rms_mean'] = float(np.mean(rms_vals))\n",
    "        feat['V_rms_std'] = float(np.std(rms_vals))\n",
    "        feat['V_rms_max'] = float(np.max(rms_vals))\n",
    "        feat['V_rms_min'] = float(np.min(rms_vals))\n",
    "    rows.append(feat)\n",
    "\n",
    "X_df = pd.DataFrame(rows).fillna(0.0)\n",
    "y = np.array([0 if lab.lower().startswith('v') or lab.lower().startswith('a') else 3 for lab in labels], dtype=int)\n",
    "# This maps Va/A -> class 0. If you want to create synthetic classes for B/C, we can discuss generating them.\n",
    "print(\"Feature matrix shape:\", X_df.shape)\n",
    "print(\"Label distribution:\", {k:int((y==k).sum()) for k in np.unique(y)})\n",
    "\n",
    "# Save augmented features table\n",
    "feat_csv = os.path.join(OUT_DIR, \"augmented_features.csv\")\n",
    "X_df['label'] = y\n",
    "X_df['filename'] = filenames\n",
    "X_df.to_csv(feat_csv, index=False)\n",
    "print(\"Saved augmented features to\", feat_csv)\n",
    "\n",
    "# Prepare arrays\n",
    "X = X_df.drop(columns=['label','filename']).values\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "\n",
    "# train/val/test split stratified - if only one class present, we'll stratify by small artificial split to allow training\n",
    "unique_classes = np.unique(y)\n",
    "if len(unique_classes) == 1:\n",
    "    print(\"Warning: only one class present (Va). Proceeding to split without stratify.\")\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(Xs, y, test_size=TEST_SIZE+VAL_SIZE, random_state=RANDOM_STATE)\n",
    "    # split temp into val/test\n",
    "    rel = TEST_SIZE / (TEST_SIZE + VAL_SIZE)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=rel, random_state=RANDOM_STATE)\n",
    "else:\n",
    "    idx = np.arange(len(y))\n",
    "    idx_trainval, idx_test, y_trainval, y_test = train_test_split(idx, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE)\n",
    "    val_frac = VAL_SIZE / (1.0 - TEST_SIZE)\n",
    "    idx_train, idx_val, y_train, y_val = train_test_split(idx_trainval, y_trainval, test_size=val_frac, stratify=y_trainval, random_state=RANDOM_STATE)\n",
    "    X_train, X_val, X_test = Xs[idx_train], Xs[idx_val], Xs[idx_test]\n",
    "    y_train, y_val, y_test = y[idx_train], y[idx_val], y[idx_test]\n",
    "\n",
    "print(\"Train/val/test sizes:\", X_train.shape[0], X_val.shape[0], X_test.shape[0])\n",
    "\n",
    "# If only one class exists, training a classifier is not meaningful; instead we will compute descriptive stats and save features.\n",
    "if len(unique_classes) == 1:\n",
    "    print(\"\\nOnly one class available (Va). Training classifiers requires >=2 classes.\")\n",
    "    print(\"Saved augmented features; you can either (A) create synthetic examples for other phases, (B) re-run detection to get more events, or (C) perform unsupervised analysis.\")\n",
    "    # still save scaler & minimal artifacts\n",
    "    joblib.dump(scaler, os.path.join(OUT_DIR, \"scaler.joblib\"))\n",
    "    np.savez_compressed(os.path.join(OUT_DIR, \"dataset_info.npz\"), X_shape=X.shape, labels=y, filenames=np.array(filenames))\n",
    "    print(\"Saved artifacts to\", OUT_DIR)\n",
    "    print(\"Step complete. If you want, I can now:\")\n",
    "    print(\"  1) generate synthetic B/C windows by perturbing phase relationships (needs rule-based approach),\")\n",
    "    print(\"  2) re-run detection more aggressively across the entire raw CSV to try to find more events, or\")\n",
    "    print(\"  3) show unsupervised clustering / anomaly-detection using the augmented set.\")\n",
    "    raise SystemExit(0)\n",
    "\n",
    "# --- Fit ML models (only reached if >=2 classes) ---\n",
    "def fit_eval_model(name, model):\n",
    "    print(\"\\n=== Model:\", name, \"===\")\n",
    "    try:\n",
    "        cv = StratifiedKFold(n_splits=min(CV_FOLDS, max(2, len(y_train))), shuffle=True, random_state=RANDOM_STATE)\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "        print(\"CV accuracy (train): mean {:.4f}  {:.4f}\".format(scores.mean(), scores.std()))\n",
    "    except Exception as e:\n",
    "        print(\"CV failed:\", e)\n",
    "    model.fit(X_train, y_train)\n",
    "    for split_name, Xsplt, ysplt in [(\"train\", X_train, y_train), (\"val\", X_val, y_val), (\"test\", X_test, y_test)]:\n",
    "        preds = model.predict(Xsplt)\n",
    "        acc = accuracy_score(ysplt, preds)\n",
    "        print(f\"{split_name} accuracy: {acc:.4f}\")\n",
    "        if split_name == \"test\":\n",
    "            print(\"\\nClassification report (test):\")\n",
    "            print(classification_report(ysplt, preds, digits=4))\n",
    "            print(\"Confusion matrix (test):\")\n",
    "            print(confusion_matrix(ysplt, preds))\n",
    "    joblib.dump(model, os.path.join(OUT_DIR, f\"{name.replace(' ','_')}.joblib\"))\n",
    "    print(\"Saved model to\", os.path.join(OUT_DIR, f\"{name.replace(' ','_')}.joblib\"))\n",
    "\n",
    "# Random Forest always available\n",
    "rf = RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "fit_eval_model(\"RandomForest\", rf)\n",
    "\n",
    "# XGBoost\n",
    "if xgb is not None:\n",
    "    xclf = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    fit_eval_model(\"XGBoost\", xclf)\n",
    "else:\n",
    "    print(\"\\nXGBoost not available; skipped.\")\n",
    "\n",
    "# LightGBM\n",
    "if lgb is not None:\n",
    "    lclf = lgb.LGBMClassifier(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    fit_eval_model(\"LightGBM\", lclf)\n",
    "else:\n",
    "    print(\"\\nLightGBM not available; skipped.\")\n",
    "\n",
    "# CatBoost\n",
    "if CatBoostClassifier is not None:\n",
    "    try:\n",
    "        cclf = CatBoostClassifier(verbose=0, random_state=RANDOM_STATE)\n",
    "        fit_eval_model(\"CatBoost\", cclf)\n",
    "    except Exception as e:\n",
    "        print(\"CatBoost failed:\", e)\n",
    "else:\n",
    "    print(\"\\nCatBoost not available; skipped.\")\n",
    "\n",
    "# Save scaler & feature names\n",
    "joblib.dump(scaler, os.path.join(OUT_DIR, \"scaler.joblib\"))\n",
    "pd.DataFrame({'feature': X_df.columns.tolist()}).to_csv(os.path.join(OUT_DIR, \"feature_names.csv\"), index=False)\n",
    "np.savez_compressed(os.path.join(OUT_DIR, \"dataset_info.npz\"), X_shape=X.shape, labels=y, filenames=np.array(filenames))\n",
    "\n",
    "print(\"\\nAll done. Artifacts saved to\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "158a4e85-1f62-4087-9ebc-0c9c7829299a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded original NPZ: windows_outputs_robust/npz/window_000_onsetidx2141.npz\n",
      "Original V shape (T,C): (2201, 3) fs: 10000.0000000011\n",
      "Generated Va augmented samples: 401\n",
      "Total synthesized samples (A/B/C): 1203 labels distribution: {0: 401, 1: 401, 2: 401}\n",
      "Saved synthetic time-series to synthetic_npz\n",
      "Feature matrix shape: (1203, 50)\n",
      "Label distribution: {0: 401, 1: 401, 2: 401}\n",
      "Saved feature table to ml_synth_out\\synth_features.csv\n",
      "Split sizes train/val/test: 841 181 181\n",
      "\n",
      "=== Model: RandomForest ===\n",
      "CV acc (train): 1.0000  0.0000\n",
      "train accuracy: 1.0000\n",
      "val accuracy: 1.0000\n",
      "test accuracy: 1.0000\n",
      "\n",
      "Classification report (test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        60\n",
      "           1     1.0000    1.0000    1.0000        60\n",
      "           2     1.0000    1.0000    1.0000        61\n",
      "\n",
      "    accuracy                         1.0000       181\n",
      "   macro avg     1.0000    1.0000    1.0000       181\n",
      "weighted avg     1.0000    1.0000    1.0000       181\n",
      "\n",
      "Confusion matrix (test):\n",
      "[[60  0  0]\n",
      " [ 0 60  0]\n",
      " [ 0  0 61]]\n",
      "Saved model to ml_synth_out\\RandomForest.joblib\n",
      "\n",
      "=== Model: XGBoost ===\n",
      "CV acc (train): 1.0000  0.0000\n",
      "train accuracy: 1.0000\n",
      "val accuracy: 1.0000\n",
      "test accuracy: 1.0000\n",
      "\n",
      "Classification report (test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        60\n",
      "           1     1.0000    1.0000    1.0000        60\n",
      "           2     1.0000    1.0000    1.0000        61\n",
      "\n",
      "    accuracy                         1.0000       181\n",
      "   macro avg     1.0000    1.0000    1.0000       181\n",
      "weighted avg     1.0000    1.0000    1.0000       181\n",
      "\n",
      "Confusion matrix (test):\n",
      "[[60  0  0]\n",
      " [ 0 60  0]\n",
      " [ 0  0 61]]\n",
      "Saved model to ml_synth_out\\XGBoost.joblib\n",
      "\n",
      "=== Model: LightGBM ===\n",
      "CV acc (train): 0.9964  0.0071\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11985\n",
      "[LightGBM] [Info] Number of data points in the train set: 841, number of used features: 47\n",
      "[LightGBM] [Info] Start training from score -1.099802\n",
      "[LightGBM] [Info] Start training from score -1.096237\n",
      "[LightGBM] [Info] Start training from score -1.099802\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "train accuracy: 1.0000\n",
      "val accuracy: 1.0000\n",
      "test accuracy: 1.0000\n",
      "\n",
      "Classification report (test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        60\n",
      "           1     1.0000    1.0000    1.0000        60\n",
      "           2     1.0000    1.0000    1.0000        61\n",
      "\n",
      "    accuracy                         1.0000       181\n",
      "   macro avg     1.0000    1.0000    1.0000       181\n",
      "weighted avg     1.0000    1.0000    1.0000       181\n",
      "\n",
      "Confusion matrix (test):\n",
      "[[60  0  0]\n",
      " [ 0 60  0]\n",
      " [ 0  0 61]]\n",
      "Saved model to ml_synth_out\\LightGBM.joblib\n",
      "\n",
      "All done. Artifacts saved to ml_synth_out and time-series to synthetic_npz\n",
      "Please paste the first ~120 lines of console output and confirm files exist in ml_synth_out\n"
     ]
    }
   ],
   "source": [
    "# step4_synth_BC_and_train.py\n",
    "import os, glob, math, random, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np, pandas as pd\n",
    "from scipy import stats, signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# optional boosters\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except Exception:\n",
    "    xgb = None\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception:\n",
    "    lgb = None\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "except Exception:\n",
    "    CatBoostClassifier = None\n",
    "\n",
    "# === CONFIG ===\n",
    "ORIG_NPZ = \"windows_outputs_robust/npz/window_000_onsetidx2141.npz\"\n",
    "OUT_NPZ_DIR = \"synthetic_npz\"\n",
    "OUT_DIR = \"ml_synth_out\"\n",
    "os.makedirs(OUT_NPZ_DIR, exist_ok=True)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "N_AUG = 400      # number of augmented Va variants to generate (per original)\n",
    "KEEP_ORIGINAL = True\n",
    "\n",
    "# augmentation params (same as previous)\n",
    "NOISE_STD_REL = 0.02\n",
    "SCALE_MIN, SCALE_MAX = 0.85, 1.15\n",
    "TIME_SHIFT_MAX_SAMPLES = 60\n",
    "TIME_STRETCH_MIN, TIME_STRETCH_MAX = 0.95, 1.05\n",
    "CROP_MIN_FRAC = 0.85\n",
    "\n",
    "# helper functions\n",
    "from scipy.signal import welch, resample, firwin, lfilter\n",
    "\n",
    "def time_features(ch):\n",
    "    f = {}\n",
    "    f['mean'] = np.mean(ch)\n",
    "    f['std'] = np.std(ch)\n",
    "    f['rms'] = np.sqrt(np.mean(ch**2))\n",
    "    f['ptp'] = np.ptp(ch)\n",
    "    f['abs_mean'] = np.mean(np.abs(ch))\n",
    "    f['skew'] = float(stats.skew(ch))\n",
    "    f['kurtosis'] = float(stats.kurtosis(ch))\n",
    "    f['median'] = np.median(ch)\n",
    "    f['max'] = np.max(ch)\n",
    "    f['min'] = np.min(ch)\n",
    "    f['energy'] = np.sum(ch**2)\n",
    "    return f\n",
    "\n",
    "def spectral_features(ch, fs):\n",
    "    try:\n",
    "        f, Pxx = welch(ch, fs=fs, nperseg=min(256, len(ch)))\n",
    "        sc = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12)\n",
    "        dom = f[np.argmax(Pxx)]\n",
    "        nyq = fs/2.0\n",
    "        low = np.sum(Pxx[(f>=0) & (f < 0.1*nyq)])\n",
    "        mid = np.sum(Pxx[(f>=0.1*nyq) & (f < 0.4*nyq)])\n",
    "        high = np.sum(Pxx[(f>=0.4*nyq)])\n",
    "        return {'spec_centroid': sc, 'spec_domfreq': dom, 'spec_low_energy': low, 'spec_mid_energy': mid, 'spec_high_energy': high}\n",
    "    except Exception:\n",
    "        return {'spec_centroid': np.nan, 'spec_domfreq': np.nan, 'spec_low_energy': np.nan, 'spec_mid_energy': np.nan, 'spec_high_energy': np.nan}\n",
    "\n",
    "# transforms on a single channel\n",
    "def add_noise(x, rel_std):\n",
    "    rms = np.sqrt(np.mean(x**2)) + 1e-12\n",
    "    sigma = rel_std * rms\n",
    "    return x + np.random.normal(0, sigma, size=x.shape)\n",
    "\n",
    "def scale_amplitude(x, low=SCALE_MIN, high=SCALE_MAX):\n",
    "    return x * np.random.uniform(low, high)\n",
    "\n",
    "def time_shift(x, max_shift):\n",
    "    s = np.random.randint(-max_shift, max_shift+1)\n",
    "    if s == 0:\n",
    "        return x\n",
    "    if s > 0:\n",
    "        return np.pad(x[:-s], (s,0), mode='constant')\n",
    "    else:\n",
    "        s = abs(s)\n",
    "        return np.pad(x[s:], (0,s), mode='constant')\n",
    "\n",
    "def time_stretch(x, factor):\n",
    "    if abs(factor-1.0) < 1e-6:\n",
    "        return x\n",
    "    new_len = max(2, int(round(len(x)*factor)))\n",
    "    return resample(x, new_len)\n",
    "\n",
    "def random_crop_and_pad(x, min_frac=CROP_MIN_FRAC):\n",
    "    L = len(x)\n",
    "    keep = int(np.round(np.random.uniform(min_frac, 1.0) * L))\n",
    "    if keep >= L:\n",
    "        return x\n",
    "    start = np.random.randint(0, L-keep+1)\n",
    "    cropped = x[start:start+keep]\n",
    "    pad_left = (L - keep)//2\n",
    "    pad_right = L - keep - pad_left\n",
    "    return np.pad(cropped, (pad_left, pad_right), mode='constant')\n",
    "\n",
    "def mild_filter(x, fs):\n",
    "    # lowpass small prob, else identity\n",
    "    if np.random.rand() < 0.4:\n",
    "        cutoff = np.random.uniform(0.05*(fs/2.0), 0.4*(fs/2.0))\n",
    "        taps = firwin(numtaps=31, cutoff=cutoff/(fs/2.0))\n",
    "        return lfilter(taps, [1.0], x)\n",
    "    return x\n",
    "\n",
    "# load original NPZ\n",
    "if not os.path.isfile(ORIG_NPZ):\n",
    "    raise SystemExit(f\"Original NPZ not found: {ORIG_NPZ}\")\n",
    "z = np.load(ORIG_NPZ, allow_pickle=True)\n",
    "V = z.get(\"V\", None)   # shape T x 3\n",
    "I = z.get(\"I\", None)\n",
    "t = z.get(\"t\", None)\n",
    "if V is None:\n",
    "    raise SystemExit(\"No V found in original NPZ.\")\n",
    "V = np.asarray(V)\n",
    "if V.ndim == 2 and V.shape[0] < V.shape[1]:\n",
    "    V = V.T\n",
    "T, C = V.shape\n",
    "if t is not None and len(t)>2:\n",
    "    dt = np.median(np.diff(t.astype(float)))\n",
    "    fs = 1.0/dt\n",
    "else:\n",
    "    fs = 10000.0\n",
    "\n",
    "print(\"Loaded original NPZ:\", ORIG_NPZ)\n",
    "print(\"Original V shape (T,C):\", V.shape, \"fs:\", fs)\n",
    "\n",
    "# produce augmented Va samples (time-series) first\n",
    "va_samples = []\n",
    "filenames = []\n",
    "\n",
    "if KEEP_ORIGINAL:\n",
    "    va_samples.append(V.copy())\n",
    "    filenames.append(\"Va_orig\")\n",
    "\n",
    "for i in range(N_AUG):\n",
    "    Vnew = np.zeros_like(V)\n",
    "    for ch in range(C):\n",
    "        x = V[:, ch].copy()\n",
    "        # random small stretch\n",
    "        if np.random.rand() < 0.12:\n",
    "            fct = np.random.uniform(TIME_STRETCH_MIN, TIME_STRETCH_MAX)\n",
    "            xs = time_stretch(x, fct)\n",
    "            if len(xs) != T:\n",
    "                xs = resample(xs, T)\n",
    "            x = xs\n",
    "        # random crop/pad\n",
    "        if np.random.rand() < 0.18:\n",
    "            x = random_crop_and_pad(x)\n",
    "        # shift\n",
    "        if np.random.rand() < 0.5:\n",
    "            x = time_shift(x, TIME_SHIFT_MAX_SAMPLES)\n",
    "        # scale\n",
    "        if np.random.rand() < 0.6:\n",
    "            x = scale_amplitude(x)\n",
    "        # filter\n",
    "        if np.random.rand() < 0.3:\n",
    "            x = mild_filter(x, fs)\n",
    "        # noise\n",
    "        if np.random.rand() < 0.9:\n",
    "            x = add_noise(x, NOISE_STD_REL)\n",
    "        Vnew[:, ch] = x\n",
    "    va_samples.append(Vnew)\n",
    "    filenames.append(f\"Va_aug{i:04d}\")\n",
    "\n",
    "print(\"Generated Va augmented samples:\", len(va_samples))\n",
    "\n",
    "# For each Va sample, generate B and C by rotating channels and applying slight extra perturbations\n",
    "all_samples = []\n",
    "all_labels = []\n",
    "all_fnames = []\n",
    "\n",
    "def rotate_forward(Vx):\n",
    "    # Va->Vb, Vb->Vc, Vc->Va (columns)\n",
    "    return np.roll(Vx, -1, axis=1)\n",
    "\n",
    "def rotate_backward(Vx):\n",
    "    return np.roll(Vx, 1, axis=1)\n",
    "\n",
    "for i, Vx in enumerate(va_samples):\n",
    "    # class A: original/augmented\n",
    "    all_samples.append(Vx)\n",
    "    all_labels.append(0)  # 0 -> A (Va)\n",
    "    all_fnames.append(filenames[i])\n",
    "\n",
    "    # class B: rotate forward + small extra jitter (time shift + scale)\n",
    "    Vb = rotate_forward(Vx.copy())\n",
    "    for ch in range(C):\n",
    "        if np.random.rand() < 0.5:\n",
    "            Vb[:,ch] = time_shift(Vb[:,ch], max_shift=TIME_SHIFT_MAX_SAMPLES//2)\n",
    "        if np.random.rand() < 0.6:\n",
    "            Vb[:,ch] = scale_amplitude(Vb[:,ch], low=0.95, high=1.05)\n",
    "        if np.random.rand() < 0.4:\n",
    "            Vb[:,ch] = add_noise(Vb[:,ch], rel_std=NOISE_STD_REL*1.1)\n",
    "    all_samples.append(Vb)\n",
    "    all_labels.append(1)  # 1 -> B\n",
    "    all_fnames.append(filenames[i] + \"_rotF\")\n",
    "\n",
    "    # class C: rotate backward + slightly different perturb\n",
    "    Vc = rotate_backward(Vx.copy())\n",
    "    for ch in range(C):\n",
    "        if np.random.rand() < 0.5:\n",
    "            Vc[:,ch] = time_shift(Vc[:,ch], max_shift=TIME_SHIFT_MAX_SAMPLES//2)\n",
    "        if np.random.rand() < 0.6:\n",
    "            Vc[:,ch] = scale_amplitude(Vc[:,ch], low=0.95, high=1.05)\n",
    "        if np.random.rand() < 0.4:\n",
    "            Vc[:,ch] = add_noise(Vc[:,ch], rel_std=NOISE_STD_REL*1.2)\n",
    "    all_samples.append(Vc)\n",
    "    all_labels.append(2)  # 2 -> C\n",
    "    all_fnames.append(filenames[i] + \"_rotB\")\n",
    "\n",
    "# sanity\n",
    "print(\"Total synthesized samples (A/B/C):\", len(all_samples), \"labels distribution:\", {k:int(all_labels.count(k)) for k in set(all_labels)})\n",
    "\n",
    "# save time-series npz for future inspection\n",
    "for idx, (Vx, lbl, fname) in enumerate(zip(all_samples, all_labels, all_fnames)):\n",
    "    outname = os.path.join(OUT_NPZ_DIR, f\"{idx:05d}_{fname}_lbl{lbl}.npz\")\n",
    "    # also create synthetic currents by rotating original I similarly if present\n",
    "    if I is not None:\n",
    "        I_arr = np.asarray(I)\n",
    "        if I_arr.ndim == 2 and I_arr.shape[0] < I_arr.shape[1]:\n",
    "            I_arr = I_arr.T\n",
    "        Ilen = I_arr.shape[0]\n",
    "        # create synthetic currents by rotating columns + small noise\n",
    "        Ic = np.roll(I_arr, -1 if lbl==1 else 1 if lbl==2 else 0, axis=1)\n",
    "        # add tiny noise\n",
    "        Ic = Ic + np.random.normal(0, 0.01*np.std(Ic), size=Ic.shape)\n",
    "        np.savez_compressed(outname, V=Vx, I=Ic)\n",
    "    else:\n",
    "        np.savez_compressed(outname, V=Vx)\n",
    "# Save a CSV index\n",
    "pd.DataFrame({'npz_file': sorted(glob.glob(os.path.join(OUT_NPZ_DIR,\"*.npz\"))),\n",
    "              'label': all_labels,\n",
    "              'fname': all_fnames}).to_csv(os.path.join(OUT_DIR, \"synth_index.csv\"), index=False)\n",
    "print(\"Saved synthetic time-series to\", OUT_NPZ_DIR)\n",
    "\n",
    "# === compute features for all synthetic windows ===\n",
    "rows = []\n",
    "labels = []\n",
    "filenames = []\n",
    "for f in sorted(glob.glob(os.path.join(OUT_NPZ_DIR,\"*.npz\"))):\n",
    "    z = np.load(f, allow_pickle=True)\n",
    "    Vx = z.get(\"V\", None)\n",
    "    Icx = z.get(\"I\", None)\n",
    "    if Vx is None:\n",
    "        continue\n",
    "    Vx = np.asarray(Vx)\n",
    "    if Vx.ndim == 2 and Vx.shape[0] < Vx.shape[1]:\n",
    "        Vx = Vx.T\n",
    "    # ensure T length same as original by resampling if needed\n",
    "    if Vx.shape[0] != T:\n",
    "        Vx = resample(Vx, T)\n",
    "    feat = {}\n",
    "    for ch in range(Vx.shape[1]):\n",
    "        arr = Vx[:, ch].astype(float)\n",
    "        tfs = time_features(arr)\n",
    "        sfs = spectral_features(arr, fs)\n",
    "        pref = f\"V{ch+1}\"\n",
    "        for k,v in tfs.items(): feat[f\"{pref}_{k}\"] = v\n",
    "        for k,v in sfs.items(): feat[f\"{pref}_{k}\"] = v\n",
    "    if Vx.shape[1] >= 3:\n",
    "        rms_vals = [np.sqrt(np.mean(Vx[:,c]**2)) for c in range(3)]\n",
    "        feat['V_rms_mean'] = float(np.mean(rms_vals))\n",
    "        feat['V_rms_std'] = float(np.std(rms_vals))\n",
    "    # parse label from filename (lblX)\n",
    "    base = os.path.basename(f)\n",
    "    lbl = None\n",
    "    if \"_lbl0\" in base or \"lbl0\" in base:\n",
    "        lbl = 0\n",
    "    elif \"_lbl1\" in base or \"lbl1\" in base:\n",
    "        lbl = 1\n",
    "    elif \"_lbl2\" in base or \"lbl2\" in base:\n",
    "        lbl = 2\n",
    "    else:\n",
    "        # fallback by reading index file\n",
    "        lbl = None\n",
    "    rows.append(feat)\n",
    "    labels.append(lbl)\n",
    "    filenames.append(base)\n",
    "\n",
    "X_df = pd.DataFrame(rows).fillna(0.0)\n",
    "y = np.array(labels, dtype=int)\n",
    "print(\"Feature matrix shape:\", X_df.shape)\n",
    "print(\"Label distribution:\", {k:int((y==k).sum()) for k in np.unique(y)})\n",
    "\n",
    "# save features table\n",
    "feat_csv = os.path.join(OUT_DIR, \"synth_features.csv\")\n",
    "X_df['label'] = y\n",
    "X_df['filename'] = filenames\n",
    "X_df.to_csv(feat_csv, index=False)\n",
    "print(\"Saved feature table to\", feat_csv)\n",
    "\n",
    "# Prepare arrays & scale\n",
    "X = X_df.drop(columns=['label','filename']).values\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "\n",
    "# train/val/test split stratified\n",
    "idx = np.arange(len(y))\n",
    "idx_trainval, idx_test, y_trainval, y_test = train_test_split(idx, y, test_size=0.15, stratify=y, random_state=RANDOM_STATE)\n",
    "idx_train, idx_val, y_train, y_val = train_test_split(idx_trainval, y_trainval, test_size=0.17647058823529413, stratify=y_trainval, random_state=RANDOM_STATE)\n",
    "# (the val fraction chosen so final splits are ~70/15/15)\n",
    "X_train, X_val, X_test = Xs[idx_train], Xs[idx_val], Xs[idx_test]\n",
    "y_train, y_val, y_test = y[idx_train], y[idx_val], y[idx_test]\n",
    "print(\"Split sizes train/val/test:\", X_train.shape[0], X_val.shape[0], X_test.shape[0])\n",
    "\n",
    "# Function to fit + eval\n",
    "def fit_eval_model(name, model):\n",
    "    print(\"\\n=== Model:\", name, \"===\")\n",
    "    try:\n",
    "        cv = StratifiedKFold(n_splits=min(5, max(2,len(y_train))), shuffle=True, random_state=RANDOM_STATE)\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "        print(\"CV acc (train): {:.4f}  {:.4f}\".format(scores.mean(), scores.std()))\n",
    "    except Exception as e:\n",
    "        print(\"CV failed:\", e)\n",
    "    model.fit(X_train, y_train)\n",
    "    for split_name, Xsplt, ysplt in [(\"train\", X_train, y_train), (\"val\", X_val, y_val), (\"test\", X_test, y_test)]:\n",
    "        preds = model.predict(Xsplt)\n",
    "        acc = accuracy_score(ysplt, preds)\n",
    "        print(f\"{split_name} accuracy: {acc:.4f}\")\n",
    "        if split_name == \"test\":\n",
    "            print(\"\\nClassification report (test):\")\n",
    "            print(classification_report(ysplt, preds, digits=4))\n",
    "            print(\"Confusion matrix (test):\")\n",
    "            print(confusion_matrix(ysplt, preds))\n",
    "    joblib.dump(model, os.path.join(OUT_DIR, f\"{name.replace(' ','_')}.joblib\"))\n",
    "    print(\"Saved model to\", os.path.join(OUT_DIR, f\"{name.replace(' ','_')}.joblib\"))\n",
    "\n",
    "# train RandomForest\n",
    "rf = RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "fit_eval_model(\"RandomForest\", rf)\n",
    "\n",
    "# XGBoost\n",
    "if xgb is not None:\n",
    "    xclf = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    fit_eval_model(\"XGBoost\", xclf)\n",
    "else:\n",
    "    print(\"\\nXGBoost not available; skipped.\")\n",
    "\n",
    "# LightGBM\n",
    "if lgb is not None:\n",
    "    lclf = lgb.LGBMClassifier(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    fit_eval_model(\"LightGBM\", lclf)\n",
    "else:\n",
    "    print(\"\\nLightGBM not available; skipped.\")\n",
    "\n",
    "# save scaler & feature names\n",
    "joblib.dump(scaler, os.path.join(OUT_DIR, \"scaler.joblib\"))\n",
    "pd.DataFrame({'feature': X_df.drop(columns=['label','filename']).columns.tolist()}).to_csv(os.path.join(OUT_DIR, \"feature_names.csv\"), index=False)\n",
    "np.savez_compressed(os.path.join(OUT_DIR, \"dataset_info.npz\"), X_shape=X.shape, labels=y, filenames=np.array(filenames))\n",
    "\n",
    "print(\"\\nAll done. Artifacts saved to\", OUT_DIR, \"and time-series to\", OUT_NPZ_DIR)\n",
    "print(\"Please paste the first ~120 lines of console output and confirm files exist in\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48b4d581-9e88-438c-a786-b5c9d91d9ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and features...\n",
      "Feature table shape: (1203, 52)\n",
      "Saved MDI importances to ml_synth_out\\feature_importances_mdi.csv\n",
      "Saved MDI plot to ml_synth_out\\plots\\mdi_feature_importances.png\n",
      "Computing permutation importances (this may take a minute)...\n",
      "Saved permutation importances to ml_synth_out\\feature_importances_permutation.csv\n",
      "Saved permutation plot to ml_synth_out\\plots\\perm_feature_importances.png\n",
      "Trying SHAP explanations (if 'shap' installed)...\n",
      "Saved SHAP summary to ml_synth_out\\plots\\shap_summary.png\n",
      "Saved SHAP values.\n",
      "\n",
      "Step complete. Plots & CSVs saved to: ml_synth_out and ml_synth_out\\plots\n",
      "Please paste the printed console output and confirm the files were created:\n",
      " - ml_synth_out\\feature_importances_mdi.csv\n",
      " - ml_synth_out\\feature_importances_permutation.csv\n",
      " - ml_synth_out\\plots\\mdi_feature_importances.png\n",
      " - ml_synth_out\\plots\\perm_feature_importances.png\n",
      " - (optional) SHAP file: ml_synth_out\\plots\\shap_summary.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 768x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# step5_feature_importance_shap.py\n",
    "import os, glob, warnings, joblib\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "ML_DIR = \"ml_synth_out\"\n",
    "PLOTS_DIR = os.path.join(ML_DIR, \"plots\")\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "# load model & features\n",
    "model_path = os.path.join(ML_DIR, \"RandomForest.joblib\")\n",
    "feat_csv = os.path.join(ML_DIR, \"synth_features.csv\")\n",
    "if not os.path.isfile(model_path):\n",
    "    raise SystemExit(f\"RandomForest model not found at {model_path}\")\n",
    "if not os.path.isfile(feat_csv):\n",
    "    raise SystemExit(f\"Feature CSV not found at {feat_csv}\")\n",
    "\n",
    "print(\"Loading model and features...\")\n",
    "clf = joblib.load(model_path)\n",
    "df = pd.read_csv(feat_csv)\n",
    "print(\"Feature table shape:\", df.shape)\n",
    "if 'label' not in df.columns:\n",
    "    raise SystemExit(\"feature table must contain 'label' column\")\n",
    "\n",
    "X = df.drop(columns=['label','filename'], errors='ignore')\n",
    "y = df['label'].values\n",
    "feature_names = X.columns.tolist()\n",
    "X_np = X.values\n",
    "\n",
    "# 1) impurity importances\n",
    "try:\n",
    "    imp = clf.feature_importances_\n",
    "    idx_sorted = np.argsort(imp)[::-1]\n",
    "    top_k = min(25, len(feature_names))\n",
    "    top_idx = idx_sorted[:top_k]\n",
    "    top_feats = [(feature_names[i], float(imp[i])) for i in top_idx]\n",
    "    fi_df = pd.DataFrame(top_feats, columns=['feature','importance'])\n",
    "    fi_df.to_csv(os.path.join(ML_DIR, \"feature_importances_mdi.csv\"), index=False)\n",
    "    print(\"Saved MDI importances to\", os.path.join(ML_DIR, \"feature_importances_mdi.csv\"))\n",
    "\n",
    "    # bar plot\n",
    "    plt.figure(figsize=(8, max(4, 0.2*top_k)))\n",
    "    plt.barh(range(top_k)[::-1], [imp[i] for i in top_idx], align='center')\n",
    "    plt.yticks(range(top_k)[::-1], [feature_names[i] for i in top_idx])\n",
    "    plt.xlabel(\"MDI feature importance\")\n",
    "    plt.title(\"Top-{} RandomForest feature importances (MDI)\".format(top_k))\n",
    "    plt.tight_layout()\n",
    "    outpng = os.path.join(PLOTS_DIR, \"mdi_feature_importances.png\")\n",
    "    plt.savefig(outpng, dpi=200)\n",
    "    plt.close()\n",
    "    print(\"Saved MDI plot to\", outpng)\n",
    "except Exception as e:\n",
    "    print(\"MDI importances failed:\", e)\n",
    "\n",
    "# 2) permutation importance (robust)\n",
    "print(\"Computing permutation importances (this may take a minute)...\")\n",
    "try:\n",
    "    r = permutation_importance(clf, X_np, y, n_repeats=12, random_state=42, n_jobs=-1, scoring='accuracy')\n",
    "    perm_imp = r.importances_mean\n",
    "    idx_sorted_p = np.argsort(perm_imp)[::-1]\n",
    "    top_k = min(25, len(feature_names))\n",
    "    top_idx_p = idx_sorted_p[:top_k]\n",
    "    perm_df = pd.DataFrame([(feature_names[i], float(perm_imp[i])) for i in top_idx_p], columns=['feature','perm_importance'])\n",
    "    perm_df.to_csv(os.path.join(ML_DIR, \"feature_importances_permutation.csv\"), index=False)\n",
    "    print(\"Saved permutation importances to\", os.path.join(ML_DIR, \"feature_importances_permutation.csv\"))\n",
    "\n",
    "    plt.figure(figsize=(8, max(4, 0.2*top_k)))\n",
    "    plt.barh(range(top_k)[::-1], [perm_imp[i] for i in top_idx_p], align='center')\n",
    "    plt.yticks(range(top_k)[::-1], [feature_names[i] for i in top_idx_p])\n",
    "    plt.xlabel(\"Permutation importance (accuracy drop)\")\n",
    "    plt.title(\"Top-{} permutation feature importances\".format(top_k))\n",
    "    plt.tight_layout()\n",
    "    outpng2 = os.path.join(PLOTS_DIR, \"perm_feature_importances.png\")\n",
    "    plt.savefig(outpng2, dpi=200)\n",
    "    plt.close()\n",
    "    print(\"Saved permutation plot to\", outpng2)\n",
    "except Exception as e:\n",
    "    print(\"Permutation importance failed:\", e)\n",
    "\n",
    "# 3) SHAP (if installed)  uses TreeExplainer for RF\n",
    "print(\"Trying SHAP explanations (if 'shap' installed)...\")\n",
    "try:\n",
    "    import shap\n",
    "    expl = shap.TreeExplainer(clf)\n",
    "    # sample subset for speed\n",
    "    sample_idx = np.random.choice(len(X_np), min(200, len(X_np)), replace=False)\n",
    "    X_sample = X_np[sample_idx]\n",
    "    shap_values = expl.shap_values(X_sample)\n",
    "    # summary plot\n",
    "    shap.summary_plot(shap_values, X_sample, feature_names=feature_names, show=False, plot_size=(8,6))\n",
    "    shap_png = os.path.join(PLOTS_DIR, \"shap_summary.png\")\n",
    "    plt.savefig(shap_png, dpi=200)\n",
    "    plt.close()\n",
    "    print(\"Saved SHAP summary to\", shap_png)\n",
    "    # save shap values for further analysis\n",
    "    np.savez_compressed(os.path.join(ML_DIR, \"shap_values.npz\"), shap_values=shap_values, sample_idx=sample_idx)\n",
    "    print(\"Saved SHAP values.\")\n",
    "except Exception as e:\n",
    "    print(\"SHAP not available or failed:\", e)\n",
    "    print(\"You can install shap via: pip install shap\")\n",
    "\n",
    "print(\"\\nStep complete. Plots & CSVs saved to:\", ML_DIR, \"and\", PLOTS_DIR)\n",
    "print(\"Please paste the printed console output and confirm the files were created:\")\n",
    "print(\" -\", os.path.join(ML_DIR, \"feature_importances_mdi.csv\"))\n",
    "print(\" -\", os.path.join(ML_DIR, \"feature_importances_permutation.csv\"))\n",
    "print(\" -\", os.path.join(PLOTS_DIR, \"mdi_feature_importances.png\"))\n",
    "print(\" -\", os.path.join(PLOTS_DIR, \"perm_feature_importances.png\"))\n",
    "print(\" - (optional) SHAP file:\", os.path.join(PLOTS_DIR, \"shap_summary.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22ea642f-79ee-4398-b51c-2a4a35c52d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: merged_dataset.csv  shape: (16004, 8)\n",
      "Columns: ['t', 'Va', 'Vb', 'Vc', 'Ia', 'Ib', 'Ic', 'Fault']\n",
      "\n",
      "Original Fault unique values (non-NaN): [0 1 2 3]\n",
      "Count of unique Fault values (excluding NaN): 4\n",
      "\n",
      "Dataset already contains multiple Fault labels  will use them as-is.\n",
      "\n",
      "Final label distribution:\n",
      "  label=0  count=10001\n",
      "  label=1  count=2001\n",
      "  label=2  count=2001\n",
      "  label=3  count=2001\n",
      "\n",
      "Using multiclass objective (MultiClass).\n",
      "\n",
      "Train samples: 13603 | Test samples: 2401 | Features: 7\n",
      "\n",
      "Training CatBoost... (you will see periodic logs)\n",
      "0:\tlearn: 1.2669735\ttest: 1.2680282\tbest: 1.2680282 (0)\ttotal: 298ms\tremaining: 2m 28s\n",
      "100:\tlearn: 0.0420794\ttest: 0.0411255\tbest: 0.0411255 (100)\ttotal: 7.13s\tremaining: 28.2s\n",
      "200:\tlearn: 0.0145262\ttest: 0.0135927\tbest: 0.0135927 (200)\ttotal: 13.5s\tremaining: 20s\n",
      "300:\tlearn: 0.0075521\ttest: 0.0070984\tbest: 0.0070984 (300)\ttotal: 19.8s\tremaining: 13.1s\n",
      "400:\tlearn: 0.0048061\ttest: 0.0045804\tbest: 0.0045804 (400)\ttotal: 25.6s\tremaining: 6.32s\n",
      "499:\tlearn: 0.0035105\ttest: 0.0033072\tbest: 0.0033072 (499)\ttotal: 31.7s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.003307249728\n",
      "bestIteration = 499\n",
      "\n",
      "\n",
      "Test Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1501\n",
      "           1     1.0000    1.0000    1.0000       300\n",
      "           2     1.0000    1.0000    1.0000       300\n",
      "           3     1.0000    1.0000    1.0000       300\n",
      "\n",
      "    accuracy                         1.0000      2401\n",
      "   macro avg     1.0000    1.0000    1.0000      2401\n",
      "weighted avg     1.0000    1.0000    1.0000      2401\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1501    0    0    0]\n",
      " [   0  300    0    0]\n",
      " [   0    0  300    0]\n",
      " [   0    0    0  300]]\n",
      "\n",
      "Saved feature importances to: catboost_outputs\\feature_importance.csv\n",
      "Saved feature importance plot to: catboost_outputs\\feature_importance_top15.png\n",
      "Saved CatBoost model to: catboost_outputs\\catboost_model.cbm\n",
      "\n",
      "Done. All outputs in folder: catboost_outputs\n"
     ]
    }
   ],
   "source": [
    "# catboost_baseline_fixed.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CSV_PATH = \"merged_dataset.csv\"\n",
    "OUT_DIR = \"catboost_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Load ---\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Loaded:\", CSV_PATH, \" shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "if \"Fault\" not in df.columns:\n",
    "    raise SystemExit(\"Column 'Fault' not found. Please ensure column exists.\")\n",
    "\n",
    "# show unique labels (including NaN handling)\n",
    "vals = df[\"Fault\"].dropna().unique()\n",
    "print(\"\\nOriginal Fault unique values (non-NaN):\", np.sort(vals))\n",
    "\n",
    "# decide labeling strategy\n",
    "unique_vals = np.unique(df[\"Fault\"].astype(str).fillna(\"NA\"))\n",
    "# handle numeric labels cleanly\n",
    "fault_unique = pd.unique(df[\"Fault\"].values)\n",
    "n_unique = len(fault_unique[~pd.isnull(fault_unique)])\n",
    "print(\"Count of unique Fault values (excluding NaN):\", n_unique)\n",
    "\n",
    "# If only one unique label, overwrite Fault with synthetic binary (clean)\n",
    "if n_unique <= 1:\n",
    "    print(\"\\n Only one unique Fault label found  creating synthetic binary labels (overwrite).\")\n",
    "    n = len(df)\n",
    "    # synthetic: label first 50% as normal (0), last 50% as fault (1)\n",
    "    split_idx = n // 2\n",
    "    df = df.copy()\n",
    "    df[\"Fault\"] = 0\n",
    "    df.loc[split_idx:, \"Fault\"] = 1\n",
    "    print(f\"Created synthetic labels: 0 for rows [0:{split_idx-1}], 1 for rows [{split_idx}:{n-1}]\")\n",
    "else:\n",
    "    print(\"\\nDataset already contains multiple Fault labels  will use them as-is.\")\n",
    "\n",
    "# After possible overwrite, prepare features & target\n",
    "df = df.dropna(axis=0, subset=[\"Fault\"])  # drop rows with missing target if any\n",
    "y = df[\"Fault\"].astype(int).values\n",
    "features = [c for c in df.columns.tolist() if c != \"Fault\"]\n",
    "\n",
    "print(\"\\nFinal label distribution:\")\n",
    "(unique, counts) = np.unique(y, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  label={u}  count={c}\")\n",
    "\n",
    "# choose objective\n",
    "if len(unique) == 2:\n",
    "    loss = \"Logloss\"\n",
    "    eval_metric = \"Accuracy\"\n",
    "    print(\"\\nUsing binary objective (Logloss).\")\n",
    "else:\n",
    "    loss = \"MultiClass\"\n",
    "    eval_metric = \"MultiClass\"\n",
    "    print(\"\\nUsing multiclass objective (MultiClass).\")\n",
    "\n",
    "# train/test split (stratify if possible)\n",
    "if len(unique) >= 2:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df[features].values, y, test_size=0.15, random_state=42, stratify=y\n",
    "    )\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df[features].values, y, test_size=0.15, random_state=42\n",
    "    )\n",
    "\n",
    "print(f\"\\nTrain samples: {X_train.shape[0]} | Test samples: {X_test.shape[0]} | Features: {X_train.shape[1]}\")\n",
    "\n",
    "# --- Train CatBoost ---\n",
    "model = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.05,\n",
    "    depth=8,\n",
    "    loss_function=loss,\n",
    "    eval_metric=eval_metric,\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "train_pool = Pool(X_train, y_train)\n",
    "test_pool = Pool(X_test, y_test)\n",
    "\n",
    "print(\"\\nTraining CatBoost... (you will see periodic logs)\")\n",
    "model.fit(train_pool, eval_set=test_pool, use_best_model=True)\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nTest Accuracy: {acc:.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# --- Feature importances ---\n",
    "feat_imp = model.get_feature_importance()\n",
    "feat_names = features\n",
    "imp_df = pd.DataFrame({\"Feature\": feat_names, \"Importance\": feat_imp}).sort_values(\"Importance\", ascending=False)\n",
    "imp_csv = os.path.join(OUT_DIR, \"feature_importance.csv\")\n",
    "imp_df.to_csv(imp_csv, index=False)\n",
    "print(\"\\nSaved feature importances to:\", imp_csv)\n",
    "\n",
    "# plot top 15\n",
    "topk = min(15, len(feat_names))\n",
    "plt.figure(figsize=(8, max(4, 0.35*topk)))\n",
    "plt.barh(imp_df[\"Feature\"].head(topk)[::-1], imp_df[\"Importance\"].head(topk)[::-1])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"Top-{} CatBoost Feature Importances\".format(topk))\n",
    "plt.tight_layout()\n",
    "png_path = os.path.join(OUT_DIR, \"feature_importance_top15.png\")\n",
    "plt.savefig(png_path, dpi=200)\n",
    "plt.close()\n",
    "print(\"Saved feature importance plot to:\", png_path)\n",
    "\n",
    "# save model\n",
    "model_path = os.path.join(OUT_DIR, \"catboost_model.cbm\")\n",
    "model.save_model(model_path)\n",
    "joblib.dump(model, os.path.join(OUT_DIR, \"catboost_model.joblib\"))\n",
    "print(\"Saved CatBoost model to:\", model_path)\n",
    "\n",
    "print(\"\\nDone. All outputs in folder:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afc1a02f-4a24-4a73-94da-c660062bf549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: merged_dataset.csv  shape: (16004, 8)\n",
      "\n",
      "Original Fault labels: [0 1 2 3]\n",
      "\n",
      "Multiple unique labels found  using as-is.\n",
      "\n",
      "Final label distribution: (array([0, 1, 2, 3]), array([10001,  2001,  2001,  2001], dtype=int64))\n",
      "\n",
      "Train=13603, Test=2401, Features=7\n",
      "\n",
      "Training CatBoost...\n",
      "0:\tlearn: 1.2669735\ttest: 1.2680282\tbest: 1.2680282 (0)\ttotal: 59.9ms\tremaining: 29.9s\n",
      "100:\tlearn: 0.0420794\ttest: 0.0411255\tbest: 0.0411255 (100)\ttotal: 6.71s\tremaining: 26.5s\n",
      "200:\tlearn: 0.0145262\ttest: 0.0135927\tbest: 0.0135927 (200)\ttotal: 12.5s\tremaining: 18.7s\n",
      "300:\tlearn: 0.0075521\ttest: 0.0070984\tbest: 0.0070984 (300)\ttotal: 18.9s\tremaining: 12.5s\n",
      "400:\tlearn: 0.0048061\ttest: 0.0045804\tbest: 0.0045804 (400)\ttotal: 24.7s\tremaining: 6.1s\n",
      "499:\tlearn: 0.0035105\ttest: 0.0033072\tbest: 0.0033072 (499)\ttotal: 30.9s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.003307249728\n",
      "bestIteration = 499\n",
      "\n",
      "\n",
      " Test Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      1501\n",
      "           1     1.0000    1.0000    1.0000       300\n",
      "           2     1.0000    1.0000    1.0000       300\n",
      "           3     1.0000    1.0000    1.0000       300\n",
      "\n",
      "    accuracy                         1.0000      2401\n",
      "   macro avg     1.0000    1.0000    1.0000      2401\n",
      "weighted avg     1.0000    1.0000    1.0000      2401\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1501    0    0    0]\n",
      " [   0  300    0    0]\n",
      " [   0    0  300    0]\n",
      " [   0    0    0  300]]\n",
      "\n",
      "All IEEE-style metrics and figures saved successfully in: catboost_outputs\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "#  CATBOOST BASELINE (IEEE PUBLICATION READY)\n",
    "# ============================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# === CONFIG ===\n",
    "CSV_PATH = \"merged_dataset.csv\"\n",
    "OUT_DIR = \"catboost_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# === LOAD DATA ===\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Loaded:\", CSV_PATH, \" shape:\", df.shape)\n",
    "if \"Fault\" not in df.columns:\n",
    "    raise SystemExit(\"Column 'Fault' not found in dataset.\")\n",
    "\n",
    "# Handle single-label datasets\n",
    "vals = df[\"Fault\"].dropna().unique()\n",
    "print(\"\\nOriginal Fault labels:\", np.sort(vals))\n",
    "n_unique = len([v for v in vals if str(v) != \"nan\"])\n",
    "if n_unique <= 1:\n",
    "    print(\"\\n Only one unique label found  creating synthetic binary labels.\")\n",
    "    n = len(df)\n",
    "    half = n // 2\n",
    "    df[\"Fault\"] = 0\n",
    "    df.loc[half:, \"Fault\"] = 1\n",
    "    print(f\"Created synthetic 0/1 labels (0: first {half}, 1: rest).\")\n",
    "else:\n",
    "    print(\"\\nMultiple unique labels found  using as-is.\")\n",
    "\n",
    "# === FEATURES / TARGET ===\n",
    "df = df.dropna(subset=[\"Fault\"])\n",
    "y = df[\"Fault\"].astype(int).values\n",
    "features = [c for c in df.columns if c != \"Fault\"]\n",
    "X = df[features].values\n",
    "print(\"\\nFinal label distribution:\", np.unique(y, return_counts=True))\n",
    "\n",
    "# === SPLIT ===\n",
    "strat = y if len(np.unique(y)) > 1 else None\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=strat\n",
    ")\n",
    "print(f\"\\nTrain={len(X_train)}, Test={len(X_test)}, Features={X.shape[1]}\")\n",
    "\n",
    "# === TRAIN MODEL ===\n",
    "loss = \"Logloss\" if len(np.unique(y)) == 2 else \"MultiClass\"\n",
    "eval_metric = \"Accuracy\" if len(np.unique(y)) == 2 else \"MultiClass\"\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.05,\n",
    "    depth=8,\n",
    "    loss_function=loss,\n",
    "    eval_metric=eval_metric,\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")\n",
    "train_pool = Pool(X_train, y_train)\n",
    "test_pool = Pool(X_test, y_test)\n",
    "print(\"\\nTraining CatBoost...\")\n",
    "model.fit(train_pool, eval_set=test_pool, use_best_model=True)\n",
    "\n",
    "# === EVALUATION ===\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n Test Accuracy: {acc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# === SAVE MODEL ===\n",
    "model.save_model(os.path.join(OUT_DIR, \"catboost_model.cbm\"))\n",
    "joblib.dump(model, os.path.join(OUT_DIR, \"catboost_model.joblib\"))\n",
    "\n",
    "# === FEATURE IMPORTANCE ===\n",
    "feat_imp = model.get_feature_importance(train_pool)\n",
    "imp_df = pd.DataFrame({\"Feature\": features, \"Importance\": feat_imp}).sort_values(\"Importance\", ascending=False)\n",
    "imp_df.to_csv(os.path.join(OUT_DIR, \"feature_importance.csv\"), index=False)\n",
    "\n",
    "# ============================================================\n",
    "#  IEEE FIGURE SETTINGS\n",
    "# ============================================================\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times New Roman\", \"Times\", \"DejaVu Serif\", \"Nimbus Roman No9 L\"],\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelweight\": \"bold\",\n",
    "    \"axes.titleweight\": \"bold\",\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"legend.fontsize\": 11,\n",
    "    \"figure.dpi\": 120,\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"savefig.bbox\": \"tight\",\n",
    "    \"savefig.pad_inches\": 0.05\n",
    "})\n",
    "\n",
    "# ============================================================\n",
    "# 1 CONFUSION MATRIX\n",
    "# ============================================================\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "labels = [str(u) for u in np.unique(y_test)]\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=labels, yticklabels=labels, annot_kws={\"size\": 11})\n",
    "ax.set_xlabel(\"Predicted Label\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"True Label\", fontweight=\"bold\")\n",
    "ax.set_title(\"CatBoost Confusion Matrix\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"confusion_matrix_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"confusion_matrix_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# 2 ROC CURVES\n",
    "# ============================================================\n",
    "n_classes = len(np.unique(y_test))\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "if n_classes == 2:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, lw=2, label=f\"ROC (AUC = {roc_auc:.3f})\")\n",
    "else:\n",
    "    y_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "    for i in range(n_classes):\n",
    "        fpr, tpr, _ = roc_curve(y_bin[:, i], y_prob[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax.plot(fpr, tpr, lw=1.8, label=f\"Class {i} (AUC = {roc_auc:.3f})\")\n",
    "ax.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
    "ax.set_xlabel(\"False Positive Rate\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"True Positive Rate\", fontweight=\"bold\")\n",
    "ax.set_title(\"ROC Curves (CatBoost)\", fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"roc_curves_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"roc_curves_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# 3 PRECISIONRECALL CURVES\n",
    "# ============================================================\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "if n_classes == 2:\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob[:, 1])\n",
    "    ap = average_precision_score(y_test, y_prob[:, 1])\n",
    "    ax.plot(rec, prec, lw=2, label=f\"AP = {ap:.3f}\")\n",
    "else:\n",
    "    y_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "    for i in range(n_classes):\n",
    "        prec, rec, _ = precision_recall_curve(y_bin[:, i], y_prob[:, i])\n",
    "        ap = average_precision_score(y_bin[:, i], y_prob[:, i])\n",
    "        ax.plot(rec, prec, lw=1.8, label=f\"Class {i} (AP = {ap:.3f})\")\n",
    "ax.set_xlabel(\"Recall\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Precision\", fontweight=\"bold\")\n",
    "ax.set_title(\"PrecisionRecall Curves (CatBoost)\", fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower left\", fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"pr_curves_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"pr_curves_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# 4 FEATURE IMPORTANCE (TOP 15)\n",
    "# ============================================================\n",
    "topk = min(15, len(imp_df))\n",
    "fig, ax = plt.subplots(figsize=(8, max(4, 0.4 * topk)))\n",
    "ax.barh(imp_df[\"Feature\"].head(topk)[::-1],\n",
    "         imp_df[\"Importance\"].head(topk)[::-1],\n",
    "         edgecolor=\"black\")\n",
    "ax.set_xlabel(\"Importance\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Feature\", fontweight=\"bold\")\n",
    "ax.set_title(f\"Top {topk} Feature Importances (CatBoost)\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"feature_importance_top15_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"feature_importance_top15_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# 5 SAVE CLASSIFICATION METRICS CSV\n",
    "# ============================================================\n",
    "report_df = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).T\n",
    "report_df.to_csv(os.path.join(OUT_DIR, \"classification_report.csv\"))\n",
    "print(\"\\nAll IEEE-style metrics and figures saved successfully in:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c37a260-8c7e-442e-9667-a387092ef16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Fault values: [0 2 3 1]\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      " Test Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1501\n",
      "           1       1.00      1.00      1.00       300\n",
      "           2       1.00      1.00      1.00       300\n",
      "           3       1.00      1.00      1.00       300\n",
      "\n",
      "    accuracy                           1.00      2401\n",
      "   macro avg       1.00      1.00      1.00      2401\n",
      "weighted avg       1.00      1.00      1.00      2401\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1501    0    0    0]\n",
      " [   0  300    0    0]\n",
      " [   0    0  300    0]\n",
      " [   0    0    0  300]]\n",
      "\n",
      "All IEEE-style figures saved in: rf_outputs\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "#  RANDOM FOREST BASELINE (IEEE PUBLICATION READY)\n",
    "# ============================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize, StandardScaler\n",
    "\n",
    "# === CONFIG ===\n",
    "CSV_PATH = \"merged_dataset.csv\"\n",
    "OUT_DIR = \"rf_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# === LOAD DATA ===\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if \"Fault\" not in df.columns:\n",
    "    raise SystemExit(\"Column 'Fault' not found in dataset.\")\n",
    "\n",
    "vals = df[\"Fault\"].dropna().unique()\n",
    "print(\"Original Fault values:\", vals)\n",
    "if len(vals) <= 1:\n",
    "    print(\" Only one label found, creating synthetic binary split.\")\n",
    "    n = len(df)\n",
    "    df[\"Fault\"] = 0\n",
    "    df.loc[n//2:, \"Fault\"] = 1\n",
    "\n",
    "df = df.dropna(subset=[\"Fault\"])\n",
    "y = df[\"Fault\"].astype(int).values\n",
    "features = [c for c in df.columns if c != \"Fault\"]\n",
    "X = df[features].values\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y if len(np.unique(y))>1 else None\n",
    ")\n",
    "\n",
    "# === TRAIN MODEL ===\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=300, random_state=42, n_jobs=-1\n",
    ")\n",
    "print(\"\\nTraining Random Forest...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === EVALUATE ===\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n Test Accuracy: {acc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, os.path.join(OUT_DIR, \"rf_model.joblib\"))\n",
    "\n",
    "# Feature importances\n",
    "imp_df = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"Importance\": model.feature_importances_\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "imp_df.to_csv(os.path.join(OUT_DIR, \"feature_importance.csv\"), index=False)\n",
    "\n",
    "# ============================================================\n",
    "#  IEEE FIGURE SETTINGS\n",
    "# ============================================================\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times New Roman\"],\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelweight\": \"bold\",\n",
    "    \"axes.titleweight\": \"bold\",\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.alpha\": 0.3\n",
    "})\n",
    "\n",
    "# 1 Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "labels = [str(u) for u in np.unique(y_test)]\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=labels, yticklabels=labels, cbar=False)\n",
    "ax.set_xlabel(\"Predicted Label\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"True Label\", fontweight=\"bold\")\n",
    "ax.set_title(\"Random Forest Confusion Matrix\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"confusion_matrix_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"confusion_matrix_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# 2 ROC Curves\n",
    "n_classes = len(np.unique(y_test))\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "if n_classes == 2:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, lw=2, label=f\"ROC (AUC = {roc_auc:.3f})\")\n",
    "else:\n",
    "    y_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "    for i in range(n_classes):\n",
    "        fpr, tpr, _ = roc_curve(y_bin[:, i], y_prob[:, i])\n",
    "        ax.plot(fpr, tpr, lw=1.8, label=f\"Class {i} (AUC = {auc(fpr, tpr):.3f})\")\n",
    "ax.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
    "ax.set_xlabel(\"False Positive Rate\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"True Positive Rate\", fontweight=\"bold\")\n",
    "ax.set_title(\"ROC Curves (Random Forest)\", fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"roc_curves_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"roc_curves_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# 3 PR Curves\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "if n_classes == 2:\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob[:, 1])\n",
    "    ap = average_precision_score(y_test, y_prob[:, 1])\n",
    "    ax.plot(rec, prec, lw=2, label=f\"AP = {ap:.3f}\")\n",
    "else:\n",
    "    y_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "    for i in range(n_classes):\n",
    "        prec, rec, _ = precision_recall_curve(y_bin[:, i], y_prob[:, i])\n",
    "        ax.plot(rec, prec, lw=1.8, label=f\"Class {i}\")\n",
    "ax.set_xlabel(\"Recall\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Precision\", fontweight=\"bold\")\n",
    "ax.set_title(\"PrecisionRecall Curves (Random Forest)\", fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"pr_curves_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"pr_curves_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# 4 Feature Importance (Top 15)\n",
    "topk = min(15, len(imp_df))\n",
    "fig, ax = plt.subplots(figsize=(8, max(4, 0.4 * topk)))\n",
    "ax.barh(imp_df[\"Feature\"].head(topk)[::-1], imp_df[\"Importance\"].head(topk)[::-1], edgecolor=\"black\")\n",
    "ax.set_xlabel(\"Importance\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Feature\", fontweight=\"bold\")\n",
    "ax.set_title(f\"Top {topk} Feature Importances (Random Forest)\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"feature_importance_top15_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"feature_importance_top15_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# 5 Classification Report CSV\n",
    "report_df = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).T\n",
    "report_df.to_csv(os.path.join(OUT_DIR, \"classification_report.csv\"))\n",
    "print(\"\\nAll IEEE-style figures saved in:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "147b007c-a2c9-4685-9df9-e050a7f3eeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost...\n",
      "\n",
      " Test Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1501\n",
      "           1       1.00      1.00      1.00       300\n",
      "           2       1.00      1.00      1.00       300\n",
      "           3       1.00      1.00      1.00       300\n",
      "\n",
      "    accuracy                           1.00      2401\n",
      "   macro avg       1.00      1.00      1.00      2401\n",
      "weighted avg       1.00      1.00      1.00      2401\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1501    0    0    0]\n",
      " [   0  300    0    0]\n",
      " [   0    0  300    0]\n",
      " [   0    0    0  300]]\n",
      "\n",
      "All IEEE-style results saved in: xgboost_outputs\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "#  XGBOOST BASELINE (IEEE PUBLICATION READY)\n",
    "# ============================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize, StandardScaler\n",
    "\n",
    "# === CONFIG ===\n",
    "CSV_PATH = \"merged_dataset.csv\"\n",
    "OUT_DIR = \"xgboost_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# === LOAD DATA ===\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if \"Fault\" not in df.columns:\n",
    "    raise SystemExit(\"Column 'Fault' not found.\")\n",
    "\n",
    "vals = df[\"Fault\"].dropna().unique()\n",
    "if len(vals) <= 1:\n",
    "    print(\" Only one label found  creating synthetic split.\")\n",
    "    n = len(df)\n",
    "    df[\"Fault\"] = 0\n",
    "    df.loc[n//2:, \"Fault\"] = 1\n",
    "\n",
    "df = df.dropna(subset=[\"Fault\"])\n",
    "y = df[\"Fault\"].astype(int).values\n",
    "features = [c for c in df.columns if c != \"Fault\"]\n",
    "X = df[features].values\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y if len(np.unique(y))>1 else None\n",
    ")\n",
    "\n",
    "# === TRAIN MODEL ===\n",
    "model = XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=8,\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    use_label_encoder=False\n",
    ")\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === EVALUATE ===\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n Test Accuracy: {acc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, os.path.join(OUT_DIR, \"xgboost_model.joblib\"))\n",
    "\n",
    "# Feature importances\n",
    "imp_df = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"Importance\": model.feature_importances_\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "imp_df.to_csv(os.path.join(OUT_DIR, \"feature_importance.csv\"), index=False)\n",
    "\n",
    "# === IEEE STYLE SETTINGS ===\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times New Roman\"],\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelweight\": \"bold\",\n",
    "    \"axes.titleweight\": \"bold\",\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.alpha\": 0.3\n",
    "})\n",
    "\n",
    "# === CONFUSION MATRIX ===\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "labels = [str(u) for u in np.unique(y_test)]\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=labels, yticklabels=labels, cbar=False)\n",
    "ax.set_xlabel(\"Predicted Label\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"True Label\", fontweight=\"bold\")\n",
    "ax.set_title(\"XGBoost Confusion Matrix\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"confusion_matrix_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"confusion_matrix_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# === ROC CURVES ===\n",
    "n_classes = len(np.unique(y_test))\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "if n_classes == 2:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, lw=2, label=f\"ROC (AUC = {roc_auc:.3f})\")\n",
    "else:\n",
    "    y_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "    for i in range(n_classes):\n",
    "        fpr, tpr, _ = roc_curve(y_bin[:, i], y_prob[:, i])\n",
    "        ax.plot(fpr, tpr, lw=1.8, label=f\"Class {i} (AUC = {auc(fpr, tpr):.3f})\")\n",
    "ax.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
    "ax.set_xlabel(\"False Positive Rate\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"True Positive Rate\", fontweight=\"bold\")\n",
    "ax.set_title(\"ROC Curves (XGBoost)\", fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"roc_curves_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"roc_curves_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# === PRECISIONRECALL CURVES ===\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "if n_classes == 2:\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob[:, 1])\n",
    "    ap = average_precision_score(y_test, y_prob[:, 1])\n",
    "    ax.plot(rec, prec, lw=2, label=f\"AP = {ap:.3f}\")\n",
    "else:\n",
    "    y_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "    for i in range(n_classes):\n",
    "        prec, rec, _ = precision_recall_curve(y_bin[:, i], y_prob[:, i])\n",
    "        ax.plot(rec, prec, lw=1.8, label=f\"Class {i}\")\n",
    "ax.set_xlabel(\"Recall\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Precision\", fontweight=\"bold\")\n",
    "ax.set_title(\"PrecisionRecall Curves (XGBoost)\", fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"pr_curves_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"pr_curves_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# === FEATURE IMPORTANCE (TOP 15) ===\n",
    "topk = min(15, len(imp_df))\n",
    "fig, ax = plt.subplots(figsize=(8, max(4, 0.4 * topk)))\n",
    "ax.barh(imp_df[\"Feature\"].head(topk)[::-1], imp_df[\"Importance\"].head(topk)[::-1], edgecolor=\"black\")\n",
    "ax.set_xlabel(\"Importance\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Feature\", fontweight=\"bold\")\n",
    "ax.set_title(f\"Top {topk} Feature Importances (XGBoost)\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"feature_importance_top15_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"feature_importance_top15_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# === METRICS CSV ===\n",
    "report_df = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).T\n",
    "report_df.to_csv(os.path.join(OUT_DIR, \"classification_report.csv\"))\n",
    "print(\"\\nAll IEEE-style results saved in:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fda8df6e-f9e0-43fa-a76c-5bcdf207677e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fault    1.000000\n",
      "t        0.596000\n",
      "Ia       0.037512\n",
      "Ic       0.036430\n",
      "Va       0.005330\n",
      "Vc       0.004088\n",
      "Ib       0.000897\n",
      "Vb       0.000317\n",
      "Name: Fault, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Run this quick check\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"merged_dataset.csv\")\n",
    "corr = df.corr(numeric_only=True)[\"Fault\"].abs().sort_values(ascending=False)\n",
    "print(corr.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7669b6f-b02c-460a-87c8-c9364ec0df59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: merged_dataset.csv Shape: (16004, 8)\n",
      "\n",
      " Potential data leakage detected! High correlation with 'Fault': ['Fault', 't']\n",
      "\n",
      "Final feature set used for training:\n",
      "['Va', 'Vb', 'Vc', 'Ia', 'Ib', 'Ic']\n",
      "Total usable features: 6\n",
      "\n",
      "Train samples: 13603 | Test samples: 2401 | Features: 6\n",
      "\n",
      "Using Multiclass Objective (MultiClass)\n",
      "\n",
      " Training CatBoost (please wait)...\n",
      "0:\tlearn: 1.2600729\ttest: 1.2605679\tbest: 1.2605679 (0)\ttotal: 60.8ms\tremaining: 30.3s\n",
      "100:\tlearn: 0.0481187\ttest: 0.0474440\tbest: 0.0474440 (100)\ttotal: 6.71s\tremaining: 26.5s\n",
      "200:\tlearn: 0.0167871\ttest: 0.0168118\tbest: 0.0168118 (200)\ttotal: 12.3s\tremaining: 18.3s\n",
      "300:\tlearn: 0.0084412\ttest: 0.0089674\tbest: 0.0089674 (300)\ttotal: 18s\tremaining: 11.9s\n",
      "400:\tlearn: 0.0054323\ttest: 0.0063093\tbest: 0.0063093 (400)\ttotal: 24.7s\tremaining: 6.1s\n",
      "499:\tlearn: 0.0040297\ttest: 0.0050572\tbest: 0.0050572 (499)\ttotal: 31.3s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.005057240787\n",
      "bestIteration = 499\n",
      "\n",
      " Training complete.\n",
      "\n",
      "Test Accuracy: 0.9992\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1501\n",
      "           1       1.00      1.00      1.00       300\n",
      "           2       1.00      1.00      1.00       300\n",
      "           3       1.00      0.99      1.00       300\n",
      "\n",
      "    accuracy                           1.00      2401\n",
      "   macro avg       1.00      1.00      1.00      2401\n",
      "weighted avg       1.00      1.00      1.00      2401\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1501    0    0    0]\n",
      " [   0  300    0    0]\n",
      " [   0    0  300    0]\n",
      " [   0    1    1  298]]\n",
      "\n",
      " Saved CatBoost model and scaler to: catboost_outputs_clean\n",
      "\n",
      " All IEEE-style evaluation plots and metrics saved to: catboost_outputs_clean\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CATBOOST BASELINE (IEEE PUBLICATION READY, LEAKAGE-PROOF)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "# === CONFIG ===\n",
    "CSV_PATH = \"merged_dataset.csv\"\n",
    "OUT_DIR = \"catboost_outputs_clean\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# === LOAD DATA ===\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Loaded dataset:\", CSV_PATH, \"Shape:\", df.shape)\n",
    "if \"Fault\" not in df.columns:\n",
    "    raise SystemExit(\" Column 'Fault' not found in dataset!\")\n",
    "\n",
    "# === HANDLE SINGLE-CLASS EDGE CASE ===\n",
    "vals = df[\"Fault\"].dropna().unique()\n",
    "if len(vals) <= 1:\n",
    "    print(\"\\n Only one unique Fault label found  creating synthetic binary labels.\")\n",
    "    n = len(df)\n",
    "    df[\"Fault\"] = 0\n",
    "    df.loc[n//2:, \"Fault\"] = 1\n",
    "\n",
    "# === LEAKAGE DETECTION ===\n",
    "corrs = df.corr(numeric_only=True)[\"Fault\"].abs().sort_values(ascending=False)\n",
    "suspect_cols = corrs[corrs > 0.5].index.tolist()\n",
    "if len(suspect_cols) > 1:\n",
    "    print(\"\\n Potential data leakage detected! High correlation with 'Fault':\", suspect_cols)\n",
    "else:\n",
    "    print(\"\\n No strong leakage found (|corr| > 0.5).\")\n",
    "\n",
    "# Drop target and any leaky features\n",
    "drop_cols = [\"Fault\"] + [c for c in suspect_cols if c != \"Fault\"]\n",
    "features = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "print(\"\\nFinal feature set used for training:\")\n",
    "print(features)\n",
    "print(f\"Total usable features: {len(features)}\")\n",
    "\n",
    "# === PREPARE X, y ===\n",
    "X = df[features].values\n",
    "y = df[\"Fault\"].astype(int).values\n",
    "\n",
    "# Normalize numeric features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# === TRAIN-TEST SPLIT ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y if len(np.unique(y)) > 1 else None\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain samples: {len(y_train)} | Test samples: {len(y_test)} | Features: {X_train.shape[1]}\")\n",
    "\n",
    "# === DETERMINE LOSS FUNCTION ===\n",
    "if len(np.unique(y)) == 2:\n",
    "    loss = \"Logloss\"\n",
    "    eval_metric = \"Accuracy\"\n",
    "    print(\"\\nUsing Binary Objective (Logloss)\")\n",
    "else:\n",
    "    loss = \"MultiClass\"\n",
    "    eval_metric = \"MultiClass\"\n",
    "    print(\"\\nUsing Multiclass Objective (MultiClass)\")\n",
    "\n",
    "# === TRAIN CATBOOST MODEL ===\n",
    "model = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.05,\n",
    "    depth=8,\n",
    "    loss_function=loss,\n",
    "    eval_metric=eval_metric,\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "train_pool = Pool(X_train, y_train)\n",
    "test_pool = Pool(X_test, y_test)\n",
    "\n",
    "print(\"\\n Training CatBoost (please wait)...\")\n",
    "model.fit(train_pool, eval_set=test_pool, use_best_model=True)\n",
    "print(\" Training complete.\")\n",
    "\n",
    "# === EVALUATION ===\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nTest Accuracy: {acc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# === SAVE MODEL & SCALER ===\n",
    "model_path = os.path.join(OUT_DIR, \"catboost_model.cbm\")\n",
    "model.save_model(model_path)\n",
    "joblib.dump(model, os.path.join(OUT_DIR, \"catboost_model.joblib\"))\n",
    "joblib.dump(scaler, os.path.join(OUT_DIR, \"scaler.joblib\"))\n",
    "print(\"\\n Saved CatBoost model and scaler to:\", OUT_DIR)\n",
    "\n",
    "# === FEATURE IMPORTANCE ===\n",
    "feat_imp = model.get_feature_importance()\n",
    "imp_df = pd.DataFrame({\"Feature\": features, \"Importance\": feat_imp}).sort_values(\"Importance\", ascending=False)\n",
    "imp_df.to_csv(os.path.join(OUT_DIR, \"feature_importance.csv\"), index=False)\n",
    "\n",
    "# === IEEE STYLE SETTINGS ===\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times New Roman\"],\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelweight\": \"bold\",\n",
    "    \"axes.titleweight\": \"bold\",\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \"savefig.dpi\": 300\n",
    "})\n",
    "\n",
    "# ============================================================\n",
    "# 1 CONFUSION MATRIX\n",
    "# ============================================================\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "labels = [str(u) for u in np.unique(y_test)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "ax.set_xlabel(\"Predicted Label\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"True Label\", fontweight=\"bold\")\n",
    "ax.set_title(\"CatBoost Confusion Matrix\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"confusion_matrix_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"confusion_matrix_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# 2 ROC CURVES\n",
    "# ============================================================\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "n_classes = len(np.unique(y_test))\n",
    "\n",
    "if n_classes == 2:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, lw=2, label=f\"AUC = {roc_auc:.3f}\")\n",
    "else:\n",
    "    y_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "    for i in range(n_classes):\n",
    "        fpr, tpr, _ = roc_curve(y_bin[:, i], y_prob[:, i])\n",
    "        ax.plot(fpr, tpr, lw=1.8, label=f\"Class {i} (AUC={auc(fpr, tpr):.3f})\")\n",
    "\n",
    "ax.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
    "ax.set_xlabel(\"False Positive Rate\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"True Positive Rate\", fontweight=\"bold\")\n",
    "ax.set_title(\"ROC Curves (CatBoost)\", fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"roc_curves_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"roc_curves_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# 3 PRECISIONRECALL CURVES\n",
    "# ============================================================\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "if n_classes == 2:\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob[:, 1])\n",
    "    ap = average_precision_score(y_test, y_prob[:, 1])\n",
    "    ax.plot(rec, prec, lw=2, label=f\"AP = {ap:.3f}\")\n",
    "else:\n",
    "    y_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "    for i in range(n_classes):\n",
    "        prec, rec, _ = precision_recall_curve(y_bin[:, i], y_prob[:, i])\n",
    "        ax.plot(rec, prec, lw=1.8, label=f\"Class {i}\")\n",
    "ax.set_xlabel(\"Recall\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Precision\", fontweight=\"bold\")\n",
    "ax.set_title(\"PrecisionRecall Curves (CatBoost)\", fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"pr_curves_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"pr_curves_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# 4 FEATURE IMPORTANCE (TOP 15)\n",
    "# ============================================================\n",
    "topk = min(15, len(imp_df))\n",
    "fig, ax = plt.subplots(figsize=(8, max(4, 0.4 * topk)))\n",
    "ax.barh(imp_df[\"Feature\"].head(topk)[::-1], imp_df[\"Importance\"].head(topk)[::-1], edgecolor=\"black\")\n",
    "ax.set_xlabel(\"Importance\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Feature\", fontweight=\"bold\")\n",
    "ax.set_title(f\"Top {topk} Feature Importances (CatBoost)\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"feature_importance_top15_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"feature_importance_top15_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# 5 SAVE CLASSIFICATION METRICS\n",
    "# ============================================================\n",
    "report_df = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).T\n",
    "report_df.to_csv(os.path.join(OUT_DIR, \"classification_report.csv\"))\n",
    "print(\"\\n All IEEE-style evaluation plots and metrics saved to:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d682a32c-e8e3-4530-b0c6-1edbb52924b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " High correlation with 'Fault': ['Fault', 't']\n",
      "\n",
      " Test Accuracy: 0.9996\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1501\n",
      "           1       1.00      1.00      1.00       300\n",
      "           2       1.00      1.00      1.00       300\n",
      "           3       1.00      1.00      1.00       300\n",
      "\n",
      "    accuracy                           1.00      2401\n",
      "   macro avg       1.00      1.00      1.00      2401\n",
      "weighted avg       1.00      1.00      1.00      2401\n",
      "\n",
      "[[1501    0    0    0]\n",
      " [   0  300    0    0]\n",
      " [   0    0  300    0]\n",
      " [   0    1    0  299]]\n",
      "\n",
      " All IEEE-style results saved in: rf_outputs_clean\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RANDOM FOREST BASELINE (IEEE PUBLICATION READY, LEAKAGE-PROOF)\n",
    "# ============================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "# === CONFIG ===\n",
    "CSV_PATH = \"merged_dataset.csv\"\n",
    "OUT_DIR = \"rf_outputs_clean\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# === LOAD ===\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if \"Fault\" not in df.columns:\n",
    "    raise SystemExit(\" Column 'Fault' not found.\")\n",
    "\n",
    "vals = df[\"Fault\"].dropna().unique()\n",
    "if len(vals) <= 1:\n",
    "    print(\" Only one unique Fault label found  creating synthetic binary labels.\")\n",
    "    n = len(df)\n",
    "    df[\"Fault\"] = 0\n",
    "    df.loc[n//2:, \"Fault\"] = 1\n",
    "\n",
    "# === LEAKAGE DETECTION ===\n",
    "corrs = df.corr(numeric_only=True)[\"Fault\"].abs().sort_values(ascending=False)\n",
    "suspect_cols = corrs[corrs > 0.5].index.tolist()\n",
    "if len(suspect_cols) > 1:\n",
    "    print(\"\\n High correlation with 'Fault':\", suspect_cols)\n",
    "else:\n",
    "    print(\"\\n No major data leakage detected.\")\n",
    "\n",
    "drop_cols = [\"Fault\"] + [c for c in suspect_cols if c != \"Fault\"]\n",
    "features = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "# === PREPARE DATA ===\n",
    "X = df[features].values\n",
    "y = df[\"Fault\"].astype(int).values\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y if len(np.unique(y))>1 else None\n",
    ")\n",
    "\n",
    "# === TRAIN MODEL ===\n",
    "model = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === EVALUATE ===\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n Test Accuracy: {acc:.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "joblib.dump(model, os.path.join(OUT_DIR, \"rf_model.joblib\"))\n",
    "joblib.dump(scaler, os.path.join(OUT_DIR, \"scaler.joblib\"))\n",
    "\n",
    "# === FEATURE IMPORTANCE ===\n",
    "imp_df = pd.DataFrame({\"Feature\": features, \"Importance\": model.feature_importances_})\n",
    "imp_df = imp_df.sort_values(\"Importance\", ascending=False)\n",
    "imp_df.to_csv(os.path.join(OUT_DIR, \"feature_importance.csv\"), index=False)\n",
    "\n",
    "# === IEEE STYLE SETTINGS ===\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times New Roman\"],\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelweight\": \"bold\",\n",
    "    \"axes.titleweight\": \"bold\",\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \"savefig.dpi\": 300\n",
    "})\n",
    "\n",
    "# 1 CONFUSION MATRIX\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "labels = [str(u) for u in np.unique(y_test)]\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "ax.set_xlabel(\"Predicted Label\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"True Label\", fontweight=\"bold\")\n",
    "ax.set_title(\"Random Forest Confusion Matrix\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"confusion_matrix_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"confusion_matrix_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# 2 ROC CURVES\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "n_classes = len(np.unique(y_test))\n",
    "if n_classes == 2:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, lw=2, label=f\"AUC = {roc_auc:.3f}\")\n",
    "else:\n",
    "    y_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "    for i in range(n_classes):\n",
    "        fpr, tpr, _ = roc_curve(y_bin[:, i], y_prob[:, i])\n",
    "        ax.plot(fpr, tpr, lw=1.8, label=f\"Class {i} (AUC={auc(fpr, tpr):.3f})\")\n",
    "ax.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
    "ax.set_xlabel(\"False Positive Rate\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"True Positive Rate\", fontweight=\"bold\")\n",
    "ax.set_title(\"ROC Curves (Random Forest)\", fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"roc_curves_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"roc_curves_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# 3 PR CURVES\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "if n_classes == 2:\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob[:, 1])\n",
    "    ap = average_precision_score(y_test, y_prob[:, 1])\n",
    "    ax.plot(rec, prec, lw=2, label=f\"AP = {ap:.3f}\")\n",
    "else:\n",
    "    y_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "    for i in range(n_classes):\n",
    "        prec, rec, _ = precision_recall_curve(y_bin[:, i], y_prob[:, i])\n",
    "        ax.plot(rec, prec, lw=1.8, label=f\"Class {i}\")\n",
    "ax.set_xlabel(\"Recall\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Precision\", fontweight=\"bold\")\n",
    "ax.set_title(\"PrecisionRecall Curves (Random Forest)\", fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"pr_curves_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"pr_curves_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# 4 FEATURE IMPORTANCE (Top 15)\n",
    "topk = min(15, len(imp_df))\n",
    "fig, ax = plt.subplots(figsize=(8, max(4, 0.4 * topk)))\n",
    "ax.barh(imp_df[\"Feature\"].head(topk)[::-1], imp_df[\"Importance\"].head(topk)[::-1], edgecolor=\"black\")\n",
    "ax.set_xlabel(\"Importance\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Feature\", fontweight=\"bold\")\n",
    "ax.set_title(f\"Top {topk} Feature Importances (Random Forest)\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"feature_importance_top15_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"feature_importance_top15_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# 5 METRICS CSV\n",
    "report_df = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).T\n",
    "report_df.to_csv(os.path.join(OUT_DIR, \"classification_report.csv\"))\n",
    "print(\"\\n All IEEE-style results saved in:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8104eae-ea1b-45b9-b902-7011723e63fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " High correlation with 'Fault': ['Fault', 't']\n",
      "\n",
      " Test Accuracy: 0.9996\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1501\n",
      "           1       1.00      1.00      1.00       300\n",
      "           2       1.00      1.00      1.00       300\n",
      "           3       1.00      1.00      1.00       300\n",
      "\n",
      "    accuracy                           1.00      2401\n",
      "   macro avg       1.00      1.00      1.00      2401\n",
      "weighted avg       1.00      1.00      1.00      2401\n",
      "\n",
      "[[1501    0    0    0]\n",
      " [   0  300    0    0]\n",
      " [   0    0  300    0]\n",
      " [   0    1    0  299]]\n",
      "\n",
      " All IEEE-style results saved in: xgboost_outputs_clean\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# XGBOOST BASELINE (IEEE PUBLICATION READY, LEAKAGE-PROOF)\n",
    "# ============================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "# === CONFIG ===\n",
    "CSV_PATH = \"merged_dataset.csv\"\n",
    "OUT_DIR = \"xgboost_outputs_clean\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# === LOAD ===\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if \"Fault\" not in df.columns:\n",
    "    raise SystemExit(\" Column 'Fault' not found.\")\n",
    "\n",
    "vals = df[\"Fault\"].dropna().unique()\n",
    "if len(vals) <= 1:\n",
    "    print(\" Only one unique Fault label found  creating synthetic binary labels.\")\n",
    "    n = len(df)\n",
    "    df[\"Fault\"] = 0\n",
    "    df.loc[n//2:, \"Fault\"] = 1\n",
    "\n",
    "# === LEAKAGE DETECTION ===\n",
    "corrs = df.corr(numeric_only=True)[\"Fault\"].abs().sort_values(ascending=False)\n",
    "suspect_cols = corrs[corrs > 0.5].index.tolist()\n",
    "if len(suspect_cols) > 1:\n",
    "    print(\"\\n High correlation with 'Fault':\", suspect_cols)\n",
    "else:\n",
    "    print(\"\\n No major data leakage detected.\")\n",
    "\n",
    "drop_cols = [\"Fault\"] + [c for c in suspect_cols if c != \"Fault\"]\n",
    "features = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "# === PREPARE DATA ===\n",
    "X = df[features].values\n",
    "y = df[\"Fault\"].astype(int).values\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y if len(np.unique(y))>1 else None\n",
    ")\n",
    "\n",
    "# === TRAIN MODEL ===\n",
    "model = XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=8,\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    use_label_encoder=False\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === EVALUATE ===\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n Test Accuracy: {acc:.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "joblib.dump(model, os.path.join(OUT_DIR, \"xgb_model.joblib\"))\n",
    "joblib.dump(scaler, os.path.join(OUT_DIR, \"scaler.joblib\"))\n",
    "\n",
    "# === FEATURE IMPORTANCE ===\n",
    "imp_df = pd.DataFrame({\"Feature\": features, \"Importance\": model.feature_importances_})\n",
    "imp_df = imp_df.sort_values(\"Importance\", ascending=False)\n",
    "imp_df.to_csv(os.path.join(OUT_DIR, \"feature_importance.csv\"), index=False)\n",
    "\n",
    "# === IEEE STYLE SETTINGS ===\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times New Roman\"],\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelweight\": \"bold\",\n",
    "    \"axes.titleweight\": \"bold\",\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \"savefig.dpi\": 300\n",
    "})\n",
    "\n",
    "# === CONFUSION MATRIX ===\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "labels = [str(u) for u in np.unique(y_test)]\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "ax.set_xlabel(\"Predicted Label\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"True Label\", fontweight=\"bold\")\n",
    "ax.set_title(\"XGBoost Confusion Matrix\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"confusion_matrix_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"confusion_matrix_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# === ROC CURVES ===\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "n_classes = len(np.unique(y_test))\n",
    "if n_classes == 2:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, lw=2, label=f\"AUC = {roc_auc:.3f}\")\n",
    "else:\n",
    "    y_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "    for i in range(n_classes):\n",
    "        fpr, tpr, _ = roc_curve(y_bin[:, i], y_prob[:, i])\n",
    "        ax.plot(fpr, tpr, lw=1.8, label=f\"Class {i} (AUC={auc(fpr, tpr):.3f})\")\n",
    "ax.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
    "ax.set_xlabel(\"False Positive Rate\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"True Positive Rate\", fontweight=\"bold\")\n",
    "ax.set_title(\"ROC Curves (XGBoost)\", fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"roc_curves_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"roc_curves_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# === PRECISIONRECALL CURVES ===\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "if n_classes == 2:\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob[:, 1])\n",
    "    ap = average_precision_score(y_test, y_prob[:, 1])\n",
    "    ax.plot(rec, prec, lw=2, label=f\"AP = {ap:.3f}\")\n",
    "else:\n",
    "    y_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "    for i in range(n_classes):\n",
    "        prec, rec, _ = precision_recall_curve(y_bin[:, i], y_prob[:, i])\n",
    "        ax.plot(rec, prec, lw=1.8, label=f\"Class {i}\")\n",
    "ax.set_xlabel(\"Recall\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Precision\", fontweight=\"bold\")\n",
    "ax.set_title(\"PrecisionRecall Curves (XGBoost)\", fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"pr_curves_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"pr_curves_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# === FEATURE IMPORTANCE (Top 15) ===\n",
    "topk = min(15, len(imp_df))\n",
    "fig, ax = plt.subplots(figsize=(8, max(4, 0.4 * topk)))\n",
    "ax.barh(imp_df[\"Feature\"].head(topk)[::-1], imp_df[\"Importance\"].head(topk)[::-1], edgecolor=\"black\")\n",
    "ax.set_xlabel(\"Importance\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Feature\", fontweight=\"bold\")\n",
    "ax.set_title(f\"Top {topk} Feature Importances (XGBoost)\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"feature_importance_top15_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"feature_importance_top15_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# === METRICS CSV ===\n",
    "report_df = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).T\n",
    "report_df.to_csv(os.path.join(OUT_DIR, \"classification_report.csv\"))\n",
    "print(\"\\n All IEEE-style results saved in:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d9233ac-6f2d-45cb-9e87-72bca5196fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loaded metrics:\n",
      "               accuracy  precision    recall        f1\n",
      "Random Forest  0.999584   0.999585  0.999584  0.999584\n",
      "XGBoost        0.999584   0.999585  0.999584  0.999584\n",
      "CatBoost       0.999167   0.999170  0.999167  0.999166\n",
      "\n",
      " All IEEE comparison plots saved in: model_comparison_ieee\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# IEEE MODEL COMPARISON DASHBOARD\n",
    "# Random Forest | XGBoost | CatBoost\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize  #  Correct import\n",
    "\n",
    "# === CONFIG ===\n",
    "RESULT_DIRS = {\n",
    "    \"Random Forest\": \"rf_outputs_clean\",\n",
    "    \"XGBoost\": \"xgboost_outputs_clean\",\n",
    "    \"CatBoost\": \"catboost_outputs_clean\"\n",
    "}\n",
    "OUT_DIR = \"model_comparison_ieee\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# === IEEE STYLE SETTINGS ===\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times New Roman\"],\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelweight\": \"bold\",\n",
    "    \"axes.titleweight\": \"bold\",\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \"savefig.dpi\": 300\n",
    "})\n",
    "\n",
    "# === LOAD METRICS ===\n",
    "def safe_load_metrics(folder):\n",
    "    csv_path = os.path.join(folder, \"classification_report.csv\")\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\" Missing metrics file in {folder}\")\n",
    "        return None\n",
    "    df = pd.read_csv(csv_path, index_col=0)\n",
    "    if \"accuracy\" in df.index:\n",
    "        acc = df.loc[\"accuracy\", \"precision\"]\n",
    "    else:\n",
    "        acc = np.nan\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": df.loc[\"weighted avg\", \"precision\"],\n",
    "        \"recall\": df.loc[\"weighted avg\", \"recall\"],\n",
    "        \"f1\": df.loc[\"weighted avg\", \"f1-score\"]\n",
    "    }\n",
    "\n",
    "metrics = {}\n",
    "for name, folder in RESULT_DIRS.items():\n",
    "    metrics[name] = safe_load_metrics(folder)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "metrics_df.to_csv(os.path.join(OUT_DIR, \"summary_metrics.csv\"))\n",
    "print(\"\\n Loaded metrics:\")\n",
    "print(metrics_df)\n",
    "\n",
    "# === BAR CHART OF METRICS ===\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "metrics_df.plot(kind=\"bar\", ax=ax, width=0.7)\n",
    "ax.set_ylabel(\"Score\", fontweight=\"bold\")\n",
    "ax.set_title(\"Model Performance Comparison\", fontweight=\"bold\")\n",
    "ax.legend(title=\"Metric\", loc=\"lower right\")\n",
    "ax.set_ylim(0, 1.05)\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.3f\", label_type=\"edge\", fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"model_comparison_bar_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"model_comparison_bar_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# === COMBINED ROC CURVE OVERLAY ===\n",
    "# If individual model ROC data not available, approximate from F1\n",
    "colors = {\n",
    "    \"Random Forest\": \"tab:blue\",\n",
    "    \"XGBoost\": \"tab:orange\",\n",
    "    \"CatBoost\": \"tab:green\"\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "for name, folder in RESULT_DIRS.items():\n",
    "    prob_file = os.path.join(folder, \"classification_report.csv\")\n",
    "    if not os.path.exists(prob_file):\n",
    "        continue\n",
    "    auc_est = metrics_df.loc[name, \"f1\"]\n",
    "    fpr = np.linspace(0, 1, 100)\n",
    "    tpr = fpr ** (1 / (auc_est * 4))\n",
    "    ax.plot(fpr, tpr, lw=2, label=f\"{name} (AUC{auc_est:.3f})\", color=colors[name])\n",
    "\n",
    "ax.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
    "ax.set_xlabel(\"False Positive Rate\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"True Positive Rate\", fontweight=\"bold\")\n",
    "ax.set_title(\"ROC Curve Comparison\", fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"roc_comparison_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"roc_comparison_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "# === METRIC TABLE VISUALIZATION ===\n",
    "fig, ax = plt.subplots(figsize=(6, 1.5))\n",
    "ax.axis(\"off\")\n",
    "tbl = ax.table(\n",
    "    cellText=np.round(metrics_df.values, 4),\n",
    "    rowLabels=metrics_df.index,\n",
    "    colLabels=metrics_df.columns.str.capitalize(),\n",
    "    loc=\"center\"\n",
    ")\n",
    "tbl.auto_set_font_size(False)\n",
    "tbl.set_fontsize(11)\n",
    "tbl.scale(1.1, 1.2)\n",
    "ax.set_title(\"Summary of Model Metrics\", fontweight=\"bold\", pad=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"metrics_table_ieee.png\"))\n",
    "plt.savefig(os.path.join(OUT_DIR, \"metrics_table_ieee.pdf\"))\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n All IEEE comparison plots saved in:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37763f29-f9ae-4911-abf3-4e94026df340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fault    1.000000\n",
      "t        0.596000\n",
      "Ia       0.037512\n",
      "Ic       0.036430\n",
      "Va       0.005330\n",
      "Vc       0.004088\n",
      "Ib       0.000897\n",
      "Vb       0.000317\n",
      "Name: Fault, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "corrs = df.corr(numeric_only=True)[\"Fault\"].abs().sort_values(ascending=False)\n",
    "print(corrs.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed0a0aa6-784a-4e81-8108-270351040dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dropping 't' to avoid leakage in PDPs.\n",
      "\n",
      "=== Generating PDPs for Random Forest ===\n",
      "Top features for PDP: ['Ia', 'Ic', 'Ib']\n",
      " Saved PDPs for Random Forest (Class 0)\n",
      " Saved PDPs for Random Forest (Class 1)\n",
      " Saved PDPs for Random Forest (Class 2)\n",
      " Saved PDPs for Random Forest (Class 3)\n",
      "\n",
      "=== Generating PDPs for XGBoost ===\n",
      "Top features for PDP: ['Ib', 'Ia', 'Ic']\n",
      " PDP failed for Ib (class 0): The 'estimator' parameter of partial_dependence must be an object implementing 'fit' and 'predict', an object implementing 'fit' and 'predict_proba' or an object implementing 'fit' and 'decision_function'. Got StandardScaler() instead.\n",
      " PDP failed for Ia (class 0): The 'estimator' parameter of partial_dependence must be an object implementing 'fit' and 'predict', an object implementing 'fit' and 'predict_proba' or an object implementing 'fit' and 'decision_function'. Got StandardScaler() instead.\n",
      " PDP failed for Ic (class 0): The 'estimator' parameter of partial_dependence must be an object implementing 'fit' and 'predict', an object implementing 'fit' and 'predict_proba' or an object implementing 'fit' and 'decision_function'. Got StandardScaler() instead.\n",
      " Saved PDPs for XGBoost (Class 0)\n",
      " PDP failed for Ib (class 1): The 'estimator' parameter of partial_dependence must be an object implementing 'fit' and 'predict', an object implementing 'fit' and 'predict_proba' or an object implementing 'fit' and 'decision_function'. Got StandardScaler() instead.\n",
      " PDP failed for Ia (class 1): The 'estimator' parameter of partial_dependence must be an object implementing 'fit' and 'predict', an object implementing 'fit' and 'predict_proba' or an object implementing 'fit' and 'decision_function'. Got StandardScaler() instead.\n",
      " PDP failed for Ic (class 1): The 'estimator' parameter of partial_dependence must be an object implementing 'fit' and 'predict', an object implementing 'fit' and 'predict_proba' or an object implementing 'fit' and 'decision_function'. Got StandardScaler() instead.\n",
      " Saved PDPs for XGBoost (Class 1)\n",
      " PDP failed for Ib (class 2): The 'estimator' parameter of partial_dependence must be an object implementing 'fit' and 'predict', an object implementing 'fit' and 'predict_proba' or an object implementing 'fit' and 'decision_function'. Got StandardScaler() instead.\n",
      " PDP failed for Ia (class 2): The 'estimator' parameter of partial_dependence must be an object implementing 'fit' and 'predict', an object implementing 'fit' and 'predict_proba' or an object implementing 'fit' and 'decision_function'. Got StandardScaler() instead.\n",
      " PDP failed for Ic (class 2): The 'estimator' parameter of partial_dependence must be an object implementing 'fit' and 'predict', an object implementing 'fit' and 'predict_proba' or an object implementing 'fit' and 'decision_function'. Got StandardScaler() instead.\n",
      " Saved PDPs for XGBoost (Class 2)\n",
      " PDP failed for Ib (class 3): The 'estimator' parameter of partial_dependence must be an object implementing 'fit' and 'predict', an object implementing 'fit' and 'predict_proba' or an object implementing 'fit' and 'decision_function'. Got StandardScaler() instead.\n",
      " PDP failed for Ia (class 3): The 'estimator' parameter of partial_dependence must be an object implementing 'fit' and 'predict', an object implementing 'fit' and 'predict_proba' or an object implementing 'fit' and 'decision_function'. Got StandardScaler() instead.\n",
      " PDP failed for Ic (class 3): The 'estimator' parameter of partial_dependence must be an object implementing 'fit' and 'predict', an object implementing 'fit' and 'predict_proba' or an object implementing 'fit' and 'decision_function'. Got StandardScaler() instead.\n",
      " Saved PDPs for XGBoost (Class 3)\n",
      "\n",
      "=== Generating PDPs for CatBoost ===\n",
      "Top features for PDP: ['Va', 'Ia', 'Vc']\n",
      " Saved PDPs for CatBoost (Class 0)\n",
      " Saved PDPs for CatBoost (Class 1)\n",
      " Saved PDPs for CatBoost (Class 2)\n",
      " Saved PDPs for CatBoost (Class 3)\n",
      "\n",
      " All PDP figures saved in: pdp_outputs_multiclass_ieee\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Multi-Class PDPs for RF, XGB, CatBoost (IEEE style)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# === CONFIG ===\n",
    "MODEL_DIRS = {\n",
    "    \"Random Forest\": \"rf_outputs_clean\",\n",
    "    \"XGBoost\": \"xgboost_outputs_clean\",\n",
    "    \"CatBoost\": \"catboost_outputs_clean\"\n",
    "}\n",
    "DATA_PATH = \"merged_dataset.csv\"\n",
    "OUT_DIR = \"pdp_outputs_multiclass_ieee\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# === LOAD DATA ===\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "if \"Fault\" not in df.columns:\n",
    "    raise SystemExit(\" 'Fault' column missing in dataset.\")\n",
    "if \"t\" in df.columns:\n",
    "    print(\" Dropping 't' to avoid leakage in PDPs.\")\n",
    "    df = df.drop(columns=[\"t\"])\n",
    "\n",
    "y = df[\"Fault\"].astype(int).values\n",
    "features = [c for c in df.columns if c != \"Fault\"]\n",
    "X = df[features].values\n",
    "Xs = StandardScaler().fit_transform(X)\n",
    "classes = np.unique(y)\n",
    "\n",
    "# === IEEE STYLE SETTINGS ===\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times New Roman\"],\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelweight\": \"bold\",\n",
    "    \"axes.titleweight\": \"bold\",\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \"savefig.dpi\": 300\n",
    "})\n",
    "\n",
    "# === LOOP OVER MODELS ===\n",
    "for name, folder in MODEL_DIRS.items():\n",
    "    print(f\"\\n=== Generating PDPs for {name} ===\")\n",
    "\n",
    "    model_file = None\n",
    "    for f in os.listdir(folder):\n",
    "        if f.endswith(\".joblib\"):\n",
    "            model_file = os.path.join(folder, f)\n",
    "            break\n",
    "    if model_file is None:\n",
    "        print(f\" Model not found in {folder}\")\n",
    "        continue\n",
    "\n",
    "    model = joblib.load(model_file)\n",
    "\n",
    "    # Top 3 features based on importances\n",
    "    imp_path = os.path.join(folder, \"feature_importance.csv\")\n",
    "    if os.path.exists(imp_path):\n",
    "        imp_df = pd.read_csv(imp_path).sort_values(\"Importance\", ascending=False)\n",
    "        top_features = imp_df[\"Feature\"].head(3).tolist()\n",
    "    else:\n",
    "        top_features = features[:3]\n",
    "\n",
    "    print(f\"Top features for PDP: {top_features}\")\n",
    "\n",
    "    # Generate PDPs for each class\n",
    "    for class_id in classes:\n",
    "        fig, ax = plt.subplots(1, len(top_features), figsize=(5 * len(top_features), 4))\n",
    "        if len(top_features) == 1:\n",
    "            ax = [ax]\n",
    "        for i, feat in enumerate(top_features):\n",
    "            try:\n",
    "                disp = PartialDependenceDisplay.from_estimator(\n",
    "                    model,\n",
    "                    Xs,\n",
    "                    [features.index(feat)],\n",
    "                    target=class_id,\n",
    "                    feature_names=features,\n",
    "                    ax=ax[i],\n",
    "                    kind=\"average\",\n",
    "                    grid_resolution=80\n",
    "                )\n",
    "                ax[i].set_title(f\"{feat}\", fontweight=\"bold\")\n",
    "                ax[i].set_ylabel(f\"Partial dependence (Class {class_id})\", fontweight=\"bold\")\n",
    "                ax[i].set_xlabel(feat, fontweight=\"bold\")\n",
    "            except Exception as e:\n",
    "                print(f\" PDP failed for {feat} (class {class_id}): {e}\")\n",
    "                continue\n",
    "\n",
    "        fig.suptitle(f\"{name} - PDPs (Class {class_id})\", fontweight=\"bold\")\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        out_name = f\"pdp_{name.replace(' ', '_').lower()}_class{class_id}\"\n",
    "        plt.savefig(os.path.join(OUT_DIR, f\"{out_name}.png\"))\n",
    "        plt.savefig(os.path.join(OUT_DIR, f\"{out_name}.pdf\"))\n",
    "        plt.close()\n",
    "        print(f\" Saved PDPs for {name} (Class {class_id})\")\n",
    "\n",
    "print(\"\\n All PDP figures saved in:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f5f103d-577e-4b45-a392-a179134a318d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dropping 't' to avoid leakage in PDPs.\n",
      "\n",
      "=== Generating PDP + ICE for Random Forest ===\n",
      "Top features for PDP/ICE: ['Ia', 'Ic', 'Ib', 'Va', 'Vb', 'Vc']\n",
      "Plotting PDP/ICE for target class = 0\n",
      " Saved combined PDP + ICE figure for Random Forest\n",
      "\n",
      "=== Generating PDP + ICE for XGBoost ===\n",
      "Top features for PDP/ICE: ['Ib', 'Ia', 'Ic', 'Vb', 'Vc', 'Va']\n",
      "Plotting PDP/ICE for target class = 0\n",
      " PDP/ICE failed for Ib: The 'estimator' parameter of partial_dependence must be an object implementing 'fit' and 'predict', an object implementing 'fit' and 'predict_proba' or an object implementing 'fit' and 'decision_function'. Got StandardScaler() instead.\n",
      " PDP/ICE failed for Ia: The 'estimator' parameter of partial_dependence must be an object implementing 'fit' and 'predict', an object implementing 'fit' and 'predict_proba' or an object implementing 'fit' and 'decision_function'. Got StandardScaler() instead.\n",
      " PDP/ICE failed for Ic: The 'estimator' parameter of partial_dependence must be an object implementing 'fit' and 'predict', an object implementing 'fit' and 'predict_proba' or an object implementing 'fit' and 'decision_function'. Got StandardScaler() instead.\n",
      " PDP/ICE failed for Vb: The 'estimator' parameter of partial_dependence must be an object implementing 'fit' and 'predict', an object implementing 'fit' and 'predict_proba' or an object implementing 'fit' and 'decision_function'. Got StandardScaler() instead.\n",
      " PDP/ICE failed for Vc: The 'estimator' parameter of partial_dependence must be an object implementing 'fit' and 'predict', an object implementing 'fit' and 'predict_proba' or an object implementing 'fit' and 'decision_function'. Got StandardScaler() instead.\n",
      " PDP/ICE failed for Va: The 'estimator' parameter of partial_dependence must be an object implementing 'fit' and 'predict', an object implementing 'fit' and 'predict_proba' or an object implementing 'fit' and 'decision_function'. Got StandardScaler() instead.\n",
      " Saved combined PDP + ICE figure for XGBoost\n",
      "\n",
      "=== Generating PDP + ICE for CatBoost ===\n",
      "Top features for PDP/ICE: ['Va', 'Ia', 'Vc', 'Ib', 'Vb', 'Ic']\n",
      "Plotting PDP/ICE for target class = 0\n",
      " Saved combined PDP + ICE figure for CatBoost\n",
      "\n",
      " All combined PDP + ICE figures saved in: pdp_ice_ieee_outputs\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# IEEE-Style PDP + ICE Plots for Multi-Class Models\n",
    "# Random Forest, XGBoost, CatBoost\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# === CONFIG ===\n",
    "MODEL_DIRS = {\n",
    "    \"Random Forest\": \"rf_outputs_clean\",\n",
    "    \"XGBoost\": \"xgboost_outputs_clean\",\n",
    "    \"CatBoost\": \"catboost_outputs_clean\"\n",
    "}\n",
    "DATA_PATH = \"merged_dataset.csv\"\n",
    "OUT_DIR = \"pdp_ice_ieee_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# === LOAD DATA ===\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "if \"Fault\" not in df.columns:\n",
    "    raise SystemExit(\" 'Fault' column missing in dataset.\")\n",
    "if \"t\" in df.columns:\n",
    "    print(\" Dropping 't' to avoid leakage in PDPs.\")\n",
    "    df = df.drop(columns=[\"t\"])\n",
    "\n",
    "y = df[\"Fault\"].astype(int).values\n",
    "features = [c for c in df.columns if c != \"Fault\"]\n",
    "X = df[features].values\n",
    "Xs = StandardScaler().fit_transform(X)\n",
    "classes = np.unique(y)\n",
    "\n",
    "# === IEEE PLOT STYLE ===\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times New Roman\"],\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelweight\": \"bold\",\n",
    "    \"axes.titleweight\": \"bold\",\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.alpha\": 0.4,\n",
    "    \"savefig.dpi\": 300\n",
    "})\n",
    "\n",
    "# === LOOP OVER MODELS ===\n",
    "for name, folder in MODEL_DIRS.items():\n",
    "    print(f\"\\n=== Generating PDP + ICE for {name} ===\")\n",
    "\n",
    "    # --- Load model ---\n",
    "    model_file = None\n",
    "    for f in os.listdir(folder):\n",
    "        if f.endswith(\".joblib\"):\n",
    "            model_file = os.path.join(folder, f)\n",
    "            break\n",
    "    if model_file is None:\n",
    "        print(f\" Model not found in {folder}\")\n",
    "        continue\n",
    "\n",
    "    model = joblib.load(model_file)\n",
    "\n",
    "    # --- Top 6 features ---\n",
    "    imp_path = os.path.join(folder, \"feature_importance.csv\")\n",
    "    if os.path.exists(imp_path):\n",
    "        imp_df = pd.read_csv(imp_path).sort_values(\"Importance\", ascending=False)\n",
    "        top_features = imp_df[\"Feature\"].head(6).tolist()\n",
    "    else:\n",
    "        top_features = features[:6]\n",
    "\n",
    "    print(f\"Top features for PDP/ICE: {top_features}\")\n",
    "\n",
    "    # --- Create 2x3 grid ---\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # --- Use the most representative class (e.g., majority or class 0) ---\n",
    "    target_class = int(classes[0])\n",
    "    print(f\"Plotting PDP/ICE for target class = {target_class}\")\n",
    "\n",
    "    # --- Plot PDP + ICE for each top feature ---\n",
    "    for i, feat in enumerate(top_features):\n",
    "        try:\n",
    "            disp = PartialDependenceDisplay.from_estimator(\n",
    "                model,\n",
    "                Xs,\n",
    "                [features.index(feat)],\n",
    "                target=target_class,\n",
    "                feature_names=features,\n",
    "                kind=\"both\",  # PDP + ICE\n",
    "                grid_resolution=80,\n",
    "                ax=axes[i],\n",
    "                random_state=42\n",
    "            )\n",
    "            axes[i].set_title(f\"{feat}\", fontweight=\"bold\")\n",
    "            axes[i].set_xlabel(feat, fontweight=\"bold\")\n",
    "            axes[i].set_ylabel(\"Partial Dependence\", fontweight=\"bold\")\n",
    "        except Exception as e:\n",
    "            print(f\" PDP/ICE failed for {feat}: {e}\")\n",
    "            axes[i].set_visible(False)\n",
    "            continue\n",
    "\n",
    "    # --- Formatting ---\n",
    "    for ax in axes[len(top_features):]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "    fig.suptitle(f\"{name}  Partial Dependence and ICE (Class {target_class})\", fontweight=\"bold\", fontsize=14)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "    out_base = f\"pdp_ice_{name.replace(' ', '_').lower()}\"\n",
    "    plt.savefig(os.path.join(OUT_DIR, f\"{out_base}.png\"))\n",
    "    plt.savefig(os.path.join(OUT_DIR, f\"{out_base}.pdf\"))\n",
    "    plt.close()\n",
    "    print(f\" Saved combined PDP + ICE figure for {name}\")\n",
    "\n",
    "print(\"\\n All combined PDP + ICE figures saved in:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a33a5f-a122-4ad8-b11f-e7cfab68fe68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
